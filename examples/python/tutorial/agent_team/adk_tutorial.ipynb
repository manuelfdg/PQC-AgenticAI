{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Np0plMPXRvoq"
      },
      "source": [
        "# Build Your First Intelligent Agent Team: A Progressive Weather Bot with ADK\n",
        "\n",
        "This tutorial extends from the [Quickstart example](https://google.github.io/adk-docs/get-started/quickstart/) for [Agent Development Kit](https://google.github.io/adk-docs/get-started/). Now, you're ready to dive deeper and construct a more sophisticated, **multi-agent system**.\n",
        "\n",
        "We'll embark on building a **Weather Bot agent team**, progressively layering advanced features onto a simple foundation. Starting with a single agent that can look up weather, we will incrementally add capabilities like:\n",
        "\n",
        "*   Leveraging different AI models (Gemini, GPT, Claude).\n",
        "*   Designing specialized sub-agents for distinct tasks (like greetings and farewells).\n",
        "*   Enabling intelligent delegation between agents.\n",
        "*   Giving agents memory using persistent session state.\n",
        "*   Implementing crucial safety guardrails using callbacks.\n",
        "\n",
        "**Why a Weather Bot Team?**\n",
        "\n",
        "This use case, while seemingly simple, provides a practical and relatable canvas to explore core ADK concepts essential for building complex, real-world agentic applications. You'll learn how to structure interactions, manage state, ensure safety, and orchestrate multiple AI \"brains\" working together.\n",
        "\n",
        "**What is ADK Again?**\n",
        "\n",
        "As a reminder, ADK is a Python framework designed to streamline the development of applications powered by Large Language Models (LLMs). It offers robust building blocks for creating agents that can reason, plan, utilize tools, interact dynamically with users, and collaborate effectively within a team.\n",
        "\n",
        "**In this advanced tutorial, you will master:**\n",
        "\n",
        "*   ✅ **Tool Definition & Usage:** Crafting Python functions (`tools`) that grant agents specific abilities (like fetching data) and instructing agents on how to use them effectively.\n",
        "*   ✅ **Multi-LLM Flexibility:** Configuring agents to utilize various leading LLMs (Gemini, GPT-4o, Claude Sonnet) via LiteLLM integration, allowing you to choose the best model for each task.\n",
        "*   ✅ **Agent Delegation & Collaboration:** Designing specialized sub-agents and enabling automatic routing (`auto flow`) of user requests to the most appropriate agent within a team.\n",
        "*   ✅ **Session State for Memory:** Utilizing `Session State` and `ToolContext` to enable agents to remember information across conversational turns, leading to more contextual interactions.\n",
        "*   ✅ **Safety Guardrails with Callbacks:** Implementing `before_model_callback` and `before_tool_callback` to inspect, modify, or block requests/tool usage based on predefined rules, enhancing application safety and control.\n",
        "\n",
        "**End State Expectation:**\n",
        "\n",
        "By completing this tutorial, you will have built a functional multi-agent Weather Bot system. This system will not only provide weather information but also handle conversational niceties, remember the last city checked, and operate within defined safety boundaries, all orchestrated using ADK.\n",
        "\n",
        "**Prerequisites:**\n",
        "\n",
        "*   ✅ **Solid understanding of Python programming.**\n",
        "*   ✅ **Familiarity with Large Language Models (LLMs), APIs, and the concept of agents.**\n",
        "*   ❗ **Crucially: Completion of the ADK Quickstart tutorial(s) or equivalent foundational knowledge of ADK basics (Agent, Runner, SessionService, basic Tool usage).** This tutorial builds directly upon those concepts.\n",
        "*   ✅ **API Keys** for the LLMs you intend to use (e.g., Google AI Studio for Gemini, OpenAI Platform, Anthropic Console).\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "**Note on Execution Environment:**\n",
        "\n",
        "This tutorial is structured for interactive notebook environments like Google Colab, Colab Enterprise, or Jupyter notebooks. Please keep the following in mind:\n",
        "\n",
        "*   **Running Async Code:** Notebook environments handle asynchronous code differently. You'll see examples using `await` (suitable when an event loop is already running, common in notebooks) or `asyncio.run()` (often needed when running as a standalone `.py` script or in specific notebook setups). The code blocks provide guidance for both scenarios.\n",
        "*   **Manual Runner/Session Setup:** The steps involve explicitly creating `Runner` and `SessionService` instances. This approach is shown because it gives you fine-grained control over the agent's execution lifecycle, session management, and state persistence.\n",
        "\n",
        "**Alternative: Using ADK's Built-in Tools (Web UI / CLI / API Server)**\n",
        "\n",
        "If you prefer a setup that handles the runner and session management automatically using ADK's standard tools, you can find the equivalent code structured for that purpose [here](https://github.com/google/adk-docs/tree/main/examples/python/tutorial/agent_team/adk-tutorial). That version is designed to be run directly with commands like `adk web` (for a web UI), `adk run` (for CLI interaction), or `adk api_server` (to expose an API). Please follow the `README.md` instructions provided in that alternative resource.\n",
        "\n",
        "---\n",
        "\n",
        "**Ready to build your agent team? Let's dive in!**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ARCoeUZCRNGi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95aa4414-def3-4c50-c032-2cdbdd9765d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installation complete.\n"
          ]
        }
      ],
      "source": [
        "# @title Step 0: Setup and Installation\n",
        "# Install ADK and LiteLLM for multi-model support\n",
        "\n",
        "!pip install google-adk -q\n",
        "!pip install litellm -q\n",
        "\n",
        "print(\"Installation complete.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "sbwxKypOSBkN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8aff70a7-12f7-4ebb-d521-d71ed4b887f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Libraries imported.\n"
          ]
        }
      ],
      "source": [
        "# @title Import necessary libraries\n",
        "import os\n",
        "import asyncio\n",
        "from google.adk.agents import Agent\n",
        "from google.adk.models.lite_llm import LiteLlm # For multi-model support\n",
        "from google.adk.sessions import InMemorySessionService\n",
        "from google.adk.runners import Runner\n",
        "from google.genai import types # For creating message Content/Parts\n",
        "\n",
        "import warnings\n",
        "# Ignore all warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import logging\n",
        "logging.basicConfig(level=logging.ERROR)\n",
        "\n",
        "print(\"Libraries imported.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "3mNsVI5eSDOi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab401d30-de78-41e4-f081-5ffab91fd37e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "API Keys Set:\n",
            "Google API Key set: Yes\n",
            "OpenAI API Key set: No (REPLACE PLACEHOLDER!)\n",
            "Anthropic API Key set: No (REPLACE PLACEHOLDER!)\n"
          ]
        }
      ],
      "source": [
        "# @title Configure API Keys (Replace with your actual keys!)\n",
        "\n",
        "# --- IMPORTANT: Replace placeholders with your real API keys ---\n",
        "from google.colab import userdata\n",
        "\n",
        "\n",
        "# Gemini API Key (Get from Google AI Studio: https://aistudio.google.com/app/apikey)\n",
        "os.environ[\"GOOGLE_API_KEY\"] = userdata.get('googleAPI') # <--- REPLACE\n",
        "\n",
        "# [Optional]\n",
        "# OpenAI API Key (Get from OpenAI Platform: https://platform.openai.com/api-keys)\n",
        "os.environ['OPENAI_API_KEY'] = 'YOUR_OPENAI_API_KEY' # <--- REPLACE\n",
        "\n",
        "# [Optional]\n",
        "# Anthropic API Key (Get from Anthropic Console: https://console.anthropic.com/settings/keys)\n",
        "os.environ['ANTHROPIC_API_KEY'] = 'YOUR_ANTHROPIC_API_KEY' # <--- REPLACE\n",
        "\n",
        "\n",
        "# --- Verify Keys (Optional Check) ---\n",
        "print(\"API Keys Set:\")\n",
        "print(f\"Google API Key set: {'Yes' if os.environ.get('GOOGLE_API_KEY') and os.environ['GOOGLE_API_KEY'] != 'YOUR_GOOGLE_API_KEY' else 'No (REPLACE PLACEHOLDER!)'}\")\n",
        "print(f\"OpenAI API Key set: {'Yes' if os.environ.get('OPENAI_API_KEY') and os.environ['OPENAI_API_KEY'] != 'YOUR_OPENAI_API_KEY' else 'No (REPLACE PLACEHOLDER!)'}\")\n",
        "print(f\"Anthropic API Key set: {'Yes' if os.environ.get('ANTHROPIC_API_KEY') and os.environ['ANTHROPIC_API_KEY'] != 'YOUR_ANTHROPIC_API_KEY' else 'No (REPLACE PLACEHOLDER!)'}\")\n",
        "\n",
        "# Configure ADK to use API keys directly (not Vertex AI for this multi-model setup)\n",
        "os.environ[\"GOOGLE_GENAI_USE_VERTEXAI\"] = \"False\"\n",
        "\n",
        "\n",
        "# @markdown **Security Note:** It's best practice to manage API keys securely (e.g., using Colab Secrets or environment variables) rather than hardcoding them directly in the notebook. Replace the placeholder strings above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "MI_qvZJrSJuR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5848b400-5660-4e68-d325-deeaba0542f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Environment configured.\n"
          ]
        }
      ],
      "source": [
        "# --- Define Model Constants for easier use ---\n",
        "\n",
        "# More supported models can be referenced here: https://ai.google.dev/gemini-api/docs/models#model-variations\n",
        "MODEL_GEMINI_2_0_FLASH = \"gemini-2.0-flash\"\n",
        "\n",
        "# More supported models can be referenced here: https://docs.litellm.ai/docs/providers/openai#openai-chat-completion-models\n",
        "# MODEL_GPT_4O = \"openai/gpt-4.1\" # You can also try: gpt-4.1-mini, gpt-4o etc.\n",
        "MODEL_GPT_4O = \"gemini-2.0-flash\"\n",
        "# More supported models can be referenced here: https://docs.litellm.ai/docs/providers/anthropic\n",
        "#MODEL_CLAUDE_SONNET = \"anthropic/claude-sonnet-4-20250514\" # You can also try: claude-opus-4-20250514 , claude-3-7-sonnet-20250219 etc\n",
        "MODEL_CLAUDE_SONNET = \"gemini-2.0-flash\"\n",
        "print(\"\\nEnvironment configured.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F7LZM3ysSOMu"
      },
      "source": [
        "---\n",
        "\n",
        "## Step 1: Your First Agent \\- Basic Weather Lookup\n",
        "\n",
        "Let's begin by building the fundamental component of our Weather Bot: a single agent capable of performing a specific task – looking up weather information. This involves creating two core pieces:\n",
        "\n",
        "1. **A Tool:** A Python function that equips the agent with the *ability* to fetch weather data.  \n",
        "2. **An Agent:** The AI \"brain\" that understands the user's request, knows it has a weather tool, and decides when and how to use it.\n",
        "\n",
        "---\n",
        "\n",
        "**1\\. Define the Tool (`get_weather`)**\n",
        "\n",
        "In ADK, **Tools** are the building blocks that give agents concrete capabilities beyond just text generation. They are typically regular Python functions that perform specific actions, like calling an API, querying a database, or performing calculations.\n",
        "\n",
        "Our first tool will provide a *mock* weather report. This allows us to focus on the agent structure without needing external API keys yet. Later, you could easily swap this mock function with one that calls a real weather service.\n",
        "\n",
        "**Key Concept: Docstrings are Crucial\\!** The agent's LLM relies heavily on the function's **docstring** to understand:\n",
        "\n",
        "* *What* the tool does.  \n",
        "* *When* to use it.  \n",
        "* *What arguments* it requires (`city: str`).  \n",
        "* *What information* it returns.\n",
        "\n",
        "**Best Practice:** Write clear, descriptive, and accurate docstrings for your tools. This is essential for the LLM to use the tool correctly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ILy7YTCbSRAT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb923ea9-2943-4613-fa9c-5f8844e509ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Tool: get_weather called for city: New York ---\n",
            "{'status': 'success', 'report': 'The weather in New York is sunny with a temperature of 25°C.'}\n",
            "--- Tool: get_weather called for city: Paris ---\n",
            "{'status': 'error', 'error_message': \"Sorry, I don't have weather information for 'Paris'.\"}\n"
          ]
        }
      ],
      "source": [
        "# @title Define the get_weather Tool\n",
        "def get_weather(city: str) -> dict:\n",
        "    \"\"\"Retrieves the current weather report for a specified city.\n",
        "\n",
        "    Args:\n",
        "        city (str): The name of the city (e.g., \"New York\", \"London\", \"Tokyo\").\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing the weather information.\n",
        "              Includes a 'status' key ('success' or 'error').\n",
        "              If 'success', includes a 'report' key with weather details.\n",
        "              If 'error', includes an 'error_message' key.\n",
        "    \"\"\"\n",
        "    print(f\"--- Tool: get_weather called for city: {city} ---\") # Log tool execution\n",
        "    city_normalized = city.lower().replace(\" \", \"\") # Basic normalization\n",
        "\n",
        "    # Mock weather data\n",
        "    mock_weather_db = {\n",
        "        \"newyork\": {\"status\": \"success\", \"report\": \"The weather in New York is sunny with a temperature of 25°C.\"},\n",
        "        \"london\": {\"status\": \"success\", \"report\": \"It's cloudy in London with a temperature of 15°C.\"},\n",
        "        \"tokyo\": {\"status\": \"success\", \"report\": \"Tokyo is experiencing light rain and a temperature of 18°C.\"},\n",
        "    }\n",
        "\n",
        "    if city_normalized in mock_weather_db:\n",
        "        return mock_weather_db[city_normalized]\n",
        "    else:\n",
        "        return {\"status\": \"error\", \"error_message\": f\"Sorry, I don't have weather information for '{city}'.\"}\n",
        "\n",
        "# Example tool usage (optional test)\n",
        "print(get_weather(\"New York\"))\n",
        "print(get_weather(\"Paris\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hAM0BqGWSTo5"
      },
      "source": [
        "---\n",
        "\n",
        "**2\\. Define the Agent (`weather_agent`)**\n",
        "\n",
        "Now, let's create the **Agent** itself. An `Agent` in ADK orchestrates the interaction between the user, the LLM, and the available tools.\n",
        "\n",
        "We configure it with several key parameters:\n",
        "\n",
        "* `name`: A unique identifier for this agent (e.g., \"weather\\_agent\\_v1\").  \n",
        "* `model`: Specifies which LLM to use (e.g., `MODEL_GEMINI_2_0_FLASH`). We'll start with a specific Gemini model.  \n",
        "* `description`: A concise summary of the agent's overall purpose. This becomes crucial later when other agents need to decide whether to delegate tasks to *this* agent.  \n",
        "* `instruction`: Detailed guidance for the LLM on how to behave, its persona, its goals, and specifically *how and when* to utilize its assigned `tools`.  \n",
        "* `tools`: A list containing the actual Python tool functions the agent is allowed to use (e.g., `[get_weather]`).\n",
        "\n",
        "**Best Practice:** Provide clear and specific `instruction` prompts. The more detailed the instructions, the better the LLM can understand its role and how to use its tools effectively. Be explicit about error handling if needed.\n",
        "\n",
        "**Best Practice:** Choose descriptive `name` and `description` values. These are used internally by ADK and are vital for features like automatic delegation (covered later)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "6Ho1COmKSUeV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9fe350c5-698f-437d-e8ee-eb88d8f7e554"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Agent 'weather_agent_v1' created using model 'gemini-2.0-flash'.\n"
          ]
        }
      ],
      "source": [
        "# @title Define the Weather Agent\n",
        "# Use one of the model constants defined earlier\n",
        "AGENT_MODEL = MODEL_GEMINI_2_0_FLASH # Starting with Gemini\n",
        "\n",
        "weather_agent = Agent(\n",
        "    name=\"weather_agent_v1\",\n",
        "    model=AGENT_MODEL, # Can be a string for Gemini or a LiteLlm object\n",
        "    description=\"Provides weather information for specific cities.\",\n",
        "    instruction=\"You are a helpful weather assistant. \"\n",
        "                \"When the user asks for the weather in a specific city, \"\n",
        "                \"use the 'get_weather' tool to find the information. \"\n",
        "                \"If the tool returns an error, inform the user politely. \"\n",
        "                \"If the tool is successful, present the weather report clearly.\",\n",
        "    tools=[get_weather], # Pass the function directly\n",
        ")\n",
        "\n",
        "print(f\"Agent '{weather_agent.name}' created using model '{AGENT_MODEL}'.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dvz7LDhbSZxL"
      },
      "source": [
        "---\n",
        "\n",
        "**3\\. Setup Runner and Session Service**\n",
        "\n",
        "To manage conversations and execute the agent, we need two more components:\n",
        "\n",
        "* `SessionService`: Responsible for managing conversation history and state for different users and sessions. The `InMemorySessionService` is a simple implementation that stores everything in memory, suitable for testing and simple applications. It keeps track of the messages exchanged. We'll explore state persistence more in Step 4\\.  \n",
        "* `Runner`: The engine that orchestrates the interaction flow. It takes user input, routes it to the appropriate agent, manages calls to the LLM and tools based on the agent's logic, handles session updates via the `SessionService`, and yields events representing the progress of the interaction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "h30dNtqMSah5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69ca5340-86e6-40a8-ddff-051c15d8cf00"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Session created: App='weather_tutorial_app', User='user_1', Session='session_001'\n",
            "Runner created for agent 'weather_agent_v1'.\n"
          ]
        }
      ],
      "source": [
        "# @title Setup Session Service and Runner\n",
        "\n",
        "# --- Session Management ---\n",
        "# Key Concept: SessionService stores conversation history & state.\n",
        "# InMemorySessionService is simple, non-persistent storage for this tutorial.\n",
        "session_service = InMemorySessionService()\n",
        "\n",
        "# Define constants for identifying the interaction context\n",
        "APP_NAME = \"weather_tutorial_app\"\n",
        "USER_ID = \"user_1\"\n",
        "SESSION_ID = \"session_001\" # Using a fixed ID for simplicity\n",
        "\n",
        "# Create the specific session where the conversation will happen\n",
        "session = await session_service.create_session(\n",
        "    app_name=APP_NAME,\n",
        "    user_id=USER_ID,\n",
        "    session_id=SESSION_ID\n",
        ")\n",
        "print(f\"Session created: App='{APP_NAME}', User='{USER_ID}', Session='{SESSION_ID}'\")\n",
        "\n",
        "# --- Runner ---\n",
        "# Key Concept: Runner orchestrates the agent execution loop.\n",
        "runner = Runner(\n",
        "    agent=weather_agent, # The agent we want to run\n",
        "    app_name=APP_NAME,   # Associates runs with our app\n",
        "    session_service=session_service # Uses our session manager\n",
        ")\n",
        "print(f\"Runner created for agent '{runner.agent.name}'.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5zKGVwRkSduA"
      },
      "source": [
        "---\n",
        "\n",
        "**4\\. Interact with the Agent**\n",
        "\n",
        "We need a way to send messages to our agent and receive its responses. Since LLM calls and tool executions can take time, ADK's `Runner` operates asynchronously.\n",
        "\n",
        "We'll define an `async` helper function (`call_agent_async`) that:\n",
        "\n",
        "1. Takes a user query string.  \n",
        "2. Packages it into the ADK `Content` format.  \n",
        "3. Calls `runner.run_async`, providing the user/session context and the new message.  \n",
        "4. Iterates through the **Events** yielded by the runner. Events represent steps in the agent's execution (e.g., tool call requested, tool result received, intermediate LLM thought, final response).  \n",
        "5. Identifies and prints the **final response** event using `event.is_final_response()`.\n",
        "\n",
        "**Why `async`?** Interactions with LLMs and potentially tools (like external APIs) are I/O-bound operations. Using `asyncio` allows the program to handle these operations efficiently without blocking execution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "yZJr8lbkSebH"
      },
      "outputs": [],
      "source": [
        "# @title Define Agent Interaction Function\n",
        "\n",
        "from google.genai import types # For creating message Content/Parts\n",
        "\n",
        "async def call_agent_async(query: str, runner, user_id, session_id):\n",
        "  \"\"\"Sends a query to the agent and prints the final response.\"\"\"\n",
        "  print(f\"\\n>>> User Query: {query}\")\n",
        "\n",
        "  # Prepare the user's message in ADK format\n",
        "  content = types.Content(role='user', parts=[types.Part(text=query)])\n",
        "\n",
        "  final_response_text = \"Agent did not produce a final response.\" # Default\n",
        "\n",
        "  # Key Concept: run_async executes the agent logic and yields Events.\n",
        "  # We iterate through events to find the final answer.\n",
        "  async for event in runner.run_async(user_id=user_id, session_id=session_id, new_message=content):\n",
        "      # You can uncomment the line below to see *all* events during execution\n",
        "      # print(f\"  [Event] Author: {event.author}, Type: {type(event).__name__}, Final: {event.is_final_response()}, Content: {event.content}\")\n",
        "\n",
        "      # Key Concept: is_final_response() marks the concluding message for the turn.\n",
        "      if event.is_final_response():\n",
        "          if event.content and event.content.parts:\n",
        "             # Assuming text response in the first part\n",
        "             final_response_text = event.content.parts[0].text\n",
        "          elif event.actions and event.actions.escalate: # Handle potential errors/escalations\n",
        "             final_response_text = f\"Agent escalated: {event.error_message or 'No specific message.'}\"\n",
        "          # Add more checks here if needed (e.g., specific error codes)\n",
        "          break # Stop processing events once the final response is found\n",
        "\n",
        "  print(f\"<<< Agent Response: {final_response_text}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z6DQSqrqk5ic"
      },
      "source": [
        "---\n",
        "\n",
        "**5\\. Run the Conversation**\n",
        "\n",
        "Finally, let's test our setup by sending a few queries to the agent. We wrap our `async` calls in a main `async` function and run it using `await`.\n",
        "\n",
        "Watch the output:\n",
        "\n",
        "* See the user queries.  \n",
        "* Notice the `--- Tool: get_weather called... ---` logs when the agent uses the tool.  \n",
        "* Observe the agent's final responses, including how it handles the case where weather data isn't available (for Paris)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "mEd2QhHyUKY8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4fd2084e-7f45-4749-a3ef-00c7184e77c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            ">>> User Query: What is the weather like in London?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:google_genai.types:Warning: there are non-text parts in the response: ['function_call'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n",
            "ERROR:asyncio:Unclosed client session\n",
            "client_session: <aiohttp.client.ClientSession object at 0x7905619c74d0>\n",
            "ERROR:asyncio:Unclosed connector\n",
            "connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x7905619d2f90>, 4212.846729676)])']\n",
            "connector: <aiohttp.connector.TCPConnector object at 0x790561c0d430>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Tool: get_weather called for city: London ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:asyncio:Unclosed client session\n",
            "client_session: <aiohttp.client.ClientSession object at 0x790561812030>\n",
            "ERROR:asyncio:Unclosed connector\n",
            "connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x7905619d3230>, 4213.367981259)])']\n",
            "connector: <aiohttp.connector.TCPConnector object at 0x7905619c79e0>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<<< Agent Response: The weather in London is cloudy with a temperature of 15°C.\n",
            "\n",
            "\n",
            ">>> User Query: How about Paris?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:google_genai.types:Warning: there are non-text parts in the response: ['function_call'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n",
            "ERROR:asyncio:Unclosed client session\n",
            "client_session: <aiohttp.client.ClientSession object at 0x790561866f90>\n",
            "ERROR:asyncio:Unclosed connector\n",
            "connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x7905619d3410>, 4214.090803518)])']\n",
            "connector: <aiohttp.connector.TCPConnector object at 0x790561866ed0>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Tool: get_weather called for city: Paris ---\n",
            "<<< Agent Response: I am sorry, I don't have weather information for Paris.\n",
            "\n",
            "\n",
            ">>> User Query: Tell me the weather in New York\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:asyncio:Unclosed client session\n",
            "client_session: <aiohttp.client.ClientSession object at 0x7905619c7f50>\n",
            "ERROR:asyncio:Unclosed connector\n",
            "connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x7905619d36b0>, 4214.899972251)])']\n",
            "connector: <aiohttp.connector.TCPConnector object at 0x790561d1e9c0>\n",
            "WARNING:google_genai.types:Warning: there are non-text parts in the response: ['function_call'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n",
            "ERROR:asyncio:Unclosed client session\n",
            "client_session: <aiohttp.client.ClientSession object at 0x790561867620>\n",
            "ERROR:asyncio:Unclosed connector\n",
            "connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x7905619d27b0>, 4215.587917401)])']\n",
            "connector: <aiohttp.connector.TCPConnector object at 0x790561867560>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Tool: get_weather called for city: New York ---\n",
            "<<< Agent Response: The weather in New York is sunny with a temperature of 25°C.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# @title Run the Initial Conversation\n",
        "\n",
        "# We need an async function to await our interaction helper\n",
        "async def run_conversation():\n",
        "    await call_agent_async(\"What is the weather like in London?\",\n",
        "                                       runner=runner,\n",
        "                                       user_id=USER_ID,\n",
        "                                       session_id=SESSION_ID)\n",
        "\n",
        "    await call_agent_async(\"How about Paris?\",\n",
        "                                       runner=runner,\n",
        "                                       user_id=USER_ID,\n",
        "                                       session_id=SESSION_ID) # Expecting the tool's error message\n",
        "\n",
        "    await call_agent_async(\"Tell me the weather in New York\",\n",
        "                                       runner=runner,\n",
        "                                       user_id=USER_ID,\n",
        "                                       session_id=SESSION_ID)\n",
        "\n",
        "# Execute the conversation using await in an async context (like Colab/Jupyter)\n",
        "await run_conversation()\n",
        "\n",
        "# --- OR ---\n",
        "\n",
        "# Uncomment the following lines if running as a standard Python script (.py file):\n",
        "# import asyncio\n",
        "# if __name__ == \"__main__\":\n",
        "#     try:\n",
        "#         asyncio.run(run_conversation())\n",
        "#     except Exception as e:\n",
        "#         print(f\"An error occurred: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xbUzAGvsmB2a"
      },
      "source": [
        "---\n",
        "\n",
        "Congratulations\\! You've successfully built and interacted with your first ADK agent. It understands the user's request, uses a tool to find information, and responds appropriately based on the tool's result.\n",
        "\n",
        "In the next step, we'll explore how to easily switch the underlying Language Model powering this agent."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HEPaI-beSh8P"
      },
      "source": [
        "## Step 2: Going Multi-Model with LiteLLM [Optional]\n",
        "\n",
        "In Step 1, we built a functional Weather Agent powered by a specific Gemini model. While effective, real-world applications often benefit from the flexibility to use *different* Large Language Models (LLMs). Why?\n",
        "\n",
        "*   **Performance:** Some models excel at specific tasks (e.g., coding, reasoning, creative writing).\n",
        "*   **Cost:** Different models have varying price points.\n",
        "*   **Capabilities:** Models offer diverse features, context window sizes, and fine-tuning options.\n",
        "*   **Availability/Redundancy:** Having alternatives ensures your application remains functional even if one provider experiences issues.\n",
        "\n",
        "ADK makes switching between models seamless through its integration with the [**LiteLLM**](https://github.com/BerriAI/litellm) library. LiteLLM acts as a consistent interface to over 100 different LLMs.\n",
        "\n",
        "**In this step, we will:**\n",
        "\n",
        "1.  Learn how to configure an ADK `Agent` to use models from providers like OpenAI (GPT) and Anthropic (Claude) using the `LiteLlm` wrapper.\n",
        "2.  Define, configure (with their own sessions and runners), and immediately test instances of our Weather Agent, each backed by a different LLM.\n",
        "3.  Interact with these different agents to observe potential variations in their responses, even when using the same underlying tool."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OvfhdrCDnPMn"
      },
      "source": [
        "---\n",
        "\n",
        "**1\\. Import `LiteLlm`**\n",
        "\n",
        "We imported this during the initial setup (Step 0), but it's the key component for multi-model support:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "mPBr56NSnMje"
      },
      "outputs": [],
      "source": [
        "# @title 1. Import LiteLlm\n",
        "from google.adk.models.lite_llm import LiteLlm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NoUbe1mZnXd9"
      },
      "source": [
        "**2\\. Define and Test Multi-Model Agents**\n",
        "\n",
        "Instead of passing only a model name string (which defaults to Google's Gemini models), we wrap the desired model identifier string within the `LiteLlm` class.\n",
        "\n",
        "*   **Key Concept: `LiteLlm` Wrapper:** The `LiteLlm(model=\"provider/model_name\")` syntax tells ADK to route requests for this agent through the LiteLLM library to the specified model provider.\n",
        "\n",
        "Make sure you have configured the necessary API keys for OpenAI and Anthropic in Step 0. We'll use the `call_agent_async` function (defined earlier, which now accepts `runner`, `user_id`, and `session_id`) to interact with each agent immediately after its setup.\n",
        "\n",
        "Each block below will:\n",
        "*   Define the agent using a specific LiteLLM model (`MODEL_GPT_4O` or `MODEL_CLAUDE_SONNET`).\n",
        "*   Create a *new, separate* `InMemorySessionService` and session specifically for that agent's test run. This keeps the conversation histories isolated for this demonstration.\n",
        "*   Create a `Runner` configured for the specific agent and its session service.\n",
        "*   Immediately call `call_agent_async` to send a query and test the agent.\n",
        "\n",
        "**Best Practice:** Use constants for model names (like `MODEL_GPT_4O`, `MODEL_CLAUDE_SONNET` defined in Step 0) to avoid typos and make code easier to manage.\n",
        "\n",
        "**Error Handling:** We wrap the agent definitions in `try...except` blocks. This prevents the entire code cell from failing if an API key for a specific provider is missing or invalid, allowing the tutorial to proceed with the models that *are* configured.\n",
        "\n",
        "First, let's create and test the agent using OpenAI's GPT-4o."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "C2WvKj4_Sp2J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31421180-f91c-4892-a1fb-a49c2ed03a56"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:asyncio:Unclosed client session\n",
            "client_session: <aiohttp.client.ClientSession object at 0x790561d1e9c0>\n",
            "ERROR:asyncio:Unclosed connector\n",
            "connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x7905619d3770>, 4216.806746585)])']\n",
            "connector: <aiohttp.connector.TCPConnector object at 0x7905618103e0>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Agent 'weather_agent_gpt' created using model 'gemini-2.0-flash'.\n",
            "Session created: App='weather_tutorial_app_gpt', User='user_1_gpt', Session='session_001_gpt'\n",
            "Runner created for agent 'weather_agent_gpt'.\n",
            "\n",
            "--- Testing GPT Agent ---\n",
            "\n",
            ">>> User Query: What's the weather in Tokyo?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[92m13:55:42 - LiteLLM:ERROR\u001b[0m: vertex_llm_base.py:500 - Failed to load vertex credentials. Check to see if credentials containing partial/invalid information. Error: (\"Failed to retrieve http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/default/?recursive=true from the Google Compute Engine metadata service. Status: 404 Response:\\nb''\", <google.auth.transport.requests._Response object at 0x7905619c77a0>)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/google/auth/compute_engine/credentials.py\", line 126, in refresh\n",
            "    self._retrieve_info(request)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/google/auth/compute_engine/credentials.py\", line 99, in _retrieve_info\n",
            "    info = _metadata.get_service_account_info(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/google/auth/compute_engine/_metadata.py\", line 338, in get_service_account_info\n",
            "    return get(request, path, params={\"recursive\": \"true\"})\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/google/auth/compute_engine/_metadata.py\", line 263, in get\n",
            "    raise exceptions.TransportError(\n",
            "google.auth.exceptions.TransportError: (\"Failed to retrieve http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/default/?recursive=true from the Google Compute Engine metadata service. Status: 404 Response:\\nb''\", <google.auth.transport.requests._Response object at 0x7905619c77a0>)\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/litellm/llms/vertex_ai/vertex_llm_base.py\", line 496, in get_access_token\n",
            "    _credentials, credential_project_id = self.load_auth(\n",
            "                                          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/litellm/llms/vertex_ai/vertex_llm_base.py\", line 120, in load_auth\n",
            "    self.refresh_auth(creds)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/litellm/llms/vertex_ai/vertex_llm_base.py\", line 251, in refresh_auth\n",
            "    credentials.refresh(Request())\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/google/auth/compute_engine/credentials.py\", line 132, in refresh\n",
            "    raise new_exc from caught_exc\n",
            "google.auth.exceptions.RefreshError: (\"Failed to retrieve http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/default/?recursive=true from the Google Compute Engine metadata service. Status: 404 Response:\\nb''\", <google.auth.transport.requests._Response object at 0x7905619c77a0>)\n",
            "ERROR:LiteLLM:Failed to load vertex credentials. Check to see if credentials containing partial/invalid information. Error: (\"Failed to retrieve http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/default/?recursive=true from the Google Compute Engine metadata service. Status: 404 Response:\\nb''\", <google.auth.transport.requests._Response object at 0x7905619c77a0>)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/google/auth/compute_engine/credentials.py\", line 126, in refresh\n",
            "    self._retrieve_info(request)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/google/auth/compute_engine/credentials.py\", line 99, in _retrieve_info\n",
            "    info = _metadata.get_service_account_info(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/google/auth/compute_engine/_metadata.py\", line 338, in get_service_account_info\n",
            "    return get(request, path, params={\"recursive\": \"true\"})\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/google/auth/compute_engine/_metadata.py\", line 263, in get\n",
            "    raise exceptions.TransportError(\n",
            "google.auth.exceptions.TransportError: (\"Failed to retrieve http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/default/?recursive=true from the Google Compute Engine metadata service. Status: 404 Response:\\nb''\", <google.auth.transport.requests._Response object at 0x7905619c77a0>)\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/litellm/llms/vertex_ai/vertex_llm_base.py\", line 496, in get_access_token\n",
            "    _credentials, credential_project_id = self.load_auth(\n",
            "                                          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/litellm/llms/vertex_ai/vertex_llm_base.py\", line 120, in load_auth\n",
            "    self.refresh_auth(creds)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/litellm/llms/vertex_ai/vertex_llm_base.py\", line 251, in refresh_auth\n",
            "    credentials.refresh(Request())\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/google/auth/compute_engine/credentials.py\", line 132, in refresh\n",
            "    raise new_exc from caught_exc\n",
            "google.auth.exceptions.RefreshError: (\"Failed to retrieve http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/default/?recursive=true from the Google Compute Engine metadata service. Status: 404 Response:\\nb''\", <google.auth.transport.requests._Response object at 0x7905619c77a0>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "❌ Could not create or run GPT agent 'gemini-2.0-flash'. Check API Key and model name. Error: litellm.APIConnectionError: (\"Failed to retrieve http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/default/?recursive=true from the Google Compute Engine metadata service. Status: 404 Response:\\nb''\", <google.auth.transport.requests._Response object at 0x7905619c77a0>)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/google/auth/compute_engine/credentials.py\", line 126, in refresh\n",
            "    self._retrieve_info(request)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/google/auth/compute_engine/credentials.py\", line 99, in _retrieve_info\n",
            "    info = _metadata.get_service_account_info(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/google/auth/compute_engine/_metadata.py\", line 338, in get_service_account_info\n",
            "    return get(request, path, params={\"recursive\": \"true\"})\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/google/auth/compute_engine/_metadata.py\", line 263, in get\n",
            "    raise exceptions.TransportError(\n",
            "google.auth.exceptions.TransportError: (\"Failed to retrieve http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/default/?recursive=true from the Google Compute Engine metadata service. Status: 404 Response:\\nb''\", <google.auth.transport.requests._Response object at 0x7905619c77a0>)\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/litellm/main.py\", line 557, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1794, in async_completion\n",
            "    _auth_header, vertex_project = await self._ensure_access_token_async(\n",
            "                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/litellm/llms/vertex_ai/vertex_llm_base.py\", line 592, in _ensure_access_token_async\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/litellm/llms/vertex_ai/vertex_llm_base.py\", line 587, in _ensure_access_token_async\n",
            "    return await asyncify(self.get_access_token)(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/asyncify.py\", line 57, in wrapper\n",
            "    return await anyio.to_thread.run_sync(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/anyio/to_thread.py\", line 56, in run_sync\n",
            "    return await get_async_backend().run_sync_in_worker_thread(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/anyio/_backends/_asyncio.py\", line 2476, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/anyio/_backends/_asyncio.py\", line 967, in run\n",
            "    result = context.run(func, *args)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/litellm/llms/vertex_ai/vertex_llm_base.py\", line 503, in get_access_token\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/litellm/llms/vertex_ai/vertex_llm_base.py\", line 496, in get_access_token\n",
            "    _credentials, credential_project_id = self.load_auth(\n",
            "                                          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/litellm/llms/vertex_ai/vertex_llm_base.py\", line 120, in load_auth\n",
            "    self.refresh_auth(creds)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/litellm/llms/vertex_ai/vertex_llm_base.py\", line 251, in refresh_auth\n",
            "    credentials.refresh(Request())\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/google/auth/compute_engine/credentials.py\", line 132, in refresh\n",
            "    raise new_exc from caught_exc\n",
            "google.auth.exceptions.RefreshError: (\"Failed to retrieve http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/default/?recursive=true from the Google Compute Engine metadata service. Status: 404 Response:\\nb''\", <google.auth.transport.requests._Response object at 0x7905619c77a0>)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# @title Define and Test GPT Agent\n",
        "\n",
        "# Make sure 'get_weather' function from Step 1 is defined in your environment.\n",
        "# Make sure 'call_agent_async' is defined from earlier.\n",
        "\n",
        "# --- Agent using GPT-4o ---\n",
        "weather_agent_gpt = None # Initialize to None\n",
        "runner_gpt = None      # Initialize runner to None\n",
        "\n",
        "try:\n",
        "    weather_agent_gpt = Agent(\n",
        "        name=\"weather_agent_gpt\",\n",
        "        # Key change: Wrap the LiteLLM model identifier\n",
        "        model=LiteLlm(model=MODEL_GPT_4O),\n",
        "        description=\"Provides weather information (using GPT-4o).\",\n",
        "        instruction=\"You are a helpful weather assistant powered by GPT-4o. \"\n",
        "                    \"Use the 'get_weather' tool for city weather requests. \"\n",
        "                    \"Clearly present successful reports or polite error messages based on the tool's output status.\",\n",
        "        tools=[get_weather], # Re-use the same tool\n",
        "    )\n",
        "    print(f\"Agent '{weather_agent_gpt.name}' created using model '{MODEL_GPT_4O}'.\")\n",
        "\n",
        "    # InMemorySessionService is simple, non-persistent storage for this tutorial.\n",
        "    session_service_gpt = InMemorySessionService() # Create a dedicated service\n",
        "\n",
        "    # Define constants for identifying the interaction context\n",
        "    APP_NAME_GPT = \"weather_tutorial_app_gpt\" # Unique app name for this test\n",
        "    USER_ID_GPT = \"user_1_gpt\"\n",
        "    SESSION_ID_GPT = \"session_001_gpt\" # Using a fixed ID for simplicity\n",
        "\n",
        "    # Create the specific session where the conversation will happen\n",
        "    session_gpt = await session_service_gpt.create_session(\n",
        "        app_name=APP_NAME_GPT,\n",
        "        user_id=USER_ID_GPT,\n",
        "        session_id=SESSION_ID_GPT\n",
        "    )\n",
        "    print(f\"Session created: App='{APP_NAME_GPT}', User='{USER_ID_GPT}', Session='{SESSION_ID_GPT}'\")\n",
        "\n",
        "    # Create a runner specific to this agent and its session service\n",
        "    runner_gpt = Runner(\n",
        "        agent=weather_agent_gpt,\n",
        "        app_name=APP_NAME_GPT,       # Use the specific app name\n",
        "        session_service=session_service_gpt # Use the specific session service\n",
        "        )\n",
        "    print(f\"Runner created for agent '{runner_gpt.agent.name}'.\")\n",
        "\n",
        "    # --- Test the GPT Agent ---\n",
        "    print(\"\\n--- Testing GPT Agent ---\")\n",
        "    # Ensure call_agent_async uses the correct runner, user_id, session_id\n",
        "    await call_agent_async(query = \"What's the weather in Tokyo?\",\n",
        "                           runner=runner_gpt,\n",
        "                           user_id=USER_ID_GPT,\n",
        "                           session_id=SESSION_ID_GPT)\n",
        "    # --- OR ---\n",
        "\n",
        "    # Uncomment the following lines if running as a standard Python script (.py file):\n",
        "    # import asyncio\n",
        "    # if __name__ == \"__main__\":\n",
        "    #     try:\n",
        "    #         asyncio.run(call_agent_async(query = \"What's the weather in Tokyo?\",\n",
        "    #                      runner=runner_gpt,\n",
        "    #                       user_id=USER_ID_GPT,\n",
        "    #                       session_id=SESSION_ID_GPT)\n",
        "    #     except Exception as e:\n",
        "    #         print(f\"An error occurred: {e}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"❌ Could not create or run GPT agent '{MODEL_GPT_4O}'. Check API Key and model name. Error: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gu_OHirKWFXN"
      },
      "source": [
        "Next, we'll do the same for Anthropic's Claude Sonnet."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "7zqJIS4_nhoh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b7ae5c1-049f-4b82-9cd3-3f60276cf4e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Agent 'weather_agent_claude' created using model 'gemini-2.0-flash'.\n",
            "Session created: App='weather_tutorial_app_claude', User='user_1_claude', Session='session_001_claude'\n",
            "Runner created for agent 'weather_agent_claude'.\n",
            "\n",
            "--- Testing Claude Agent ---\n",
            "\n",
            ">>> User Query: Weather in London please.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[92m13:55:42 - LiteLLM:ERROR\u001b[0m: vertex_llm_base.py:500 - Failed to load vertex credentials. Check to see if credentials containing partial/invalid information. Error: (\"Failed to retrieve http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/default/?recursive=true from the Google Compute Engine metadata service. Status: 404 Response:\\nb''\", <google.auth.transport.requests._Response object at 0x7905619c74d0>)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/google/auth/compute_engine/credentials.py\", line 126, in refresh\n",
            "    self._retrieve_info(request)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/google/auth/compute_engine/credentials.py\", line 99, in _retrieve_info\n",
            "    info = _metadata.get_service_account_info(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/google/auth/compute_engine/_metadata.py\", line 338, in get_service_account_info\n",
            "    return get(request, path, params={\"recursive\": \"true\"})\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/google/auth/compute_engine/_metadata.py\", line 263, in get\n",
            "    raise exceptions.TransportError(\n",
            "google.auth.exceptions.TransportError: (\"Failed to retrieve http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/default/?recursive=true from the Google Compute Engine metadata service. Status: 404 Response:\\nb''\", <google.auth.transport.requests._Response object at 0x7905619c74d0>)\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/litellm/llms/vertex_ai/vertex_llm_base.py\", line 496, in get_access_token\n",
            "    _credentials, credential_project_id = self.load_auth(\n",
            "                                          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/litellm/llms/vertex_ai/vertex_llm_base.py\", line 120, in load_auth\n",
            "    self.refresh_auth(creds)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/litellm/llms/vertex_ai/vertex_llm_base.py\", line 251, in refresh_auth\n",
            "    credentials.refresh(Request())\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/google/auth/compute_engine/credentials.py\", line 132, in refresh\n",
            "    raise new_exc from caught_exc\n",
            "google.auth.exceptions.RefreshError: (\"Failed to retrieve http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/default/?recursive=true from the Google Compute Engine metadata service. Status: 404 Response:\\nb''\", <google.auth.transport.requests._Response object at 0x7905619c74d0>)\n",
            "ERROR:LiteLLM:Failed to load vertex credentials. Check to see if credentials containing partial/invalid information. Error: (\"Failed to retrieve http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/default/?recursive=true from the Google Compute Engine metadata service. Status: 404 Response:\\nb''\", <google.auth.transport.requests._Response object at 0x7905619c74d0>)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/google/auth/compute_engine/credentials.py\", line 126, in refresh\n",
            "    self._retrieve_info(request)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/google/auth/compute_engine/credentials.py\", line 99, in _retrieve_info\n",
            "    info = _metadata.get_service_account_info(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/google/auth/compute_engine/_metadata.py\", line 338, in get_service_account_info\n",
            "    return get(request, path, params={\"recursive\": \"true\"})\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/google/auth/compute_engine/_metadata.py\", line 263, in get\n",
            "    raise exceptions.TransportError(\n",
            "google.auth.exceptions.TransportError: (\"Failed to retrieve http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/default/?recursive=true from the Google Compute Engine metadata service. Status: 404 Response:\\nb''\", <google.auth.transport.requests._Response object at 0x7905619c74d0>)\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/litellm/llms/vertex_ai/vertex_llm_base.py\", line 496, in get_access_token\n",
            "    _credentials, credential_project_id = self.load_auth(\n",
            "                                          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/litellm/llms/vertex_ai/vertex_llm_base.py\", line 120, in load_auth\n",
            "    self.refresh_auth(creds)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/litellm/llms/vertex_ai/vertex_llm_base.py\", line 251, in refresh_auth\n",
            "    credentials.refresh(Request())\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/google/auth/compute_engine/credentials.py\", line 132, in refresh\n",
            "    raise new_exc from caught_exc\n",
            "google.auth.exceptions.RefreshError: (\"Failed to retrieve http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/default/?recursive=true from the Google Compute Engine metadata service. Status: 404 Response:\\nb''\", <google.auth.transport.requests._Response object at 0x7905619c74d0>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "❌ Could not create or run Claude agent 'gemini-2.0-flash'. Check API Key and model name. Error: litellm.APIConnectionError: (\"Failed to retrieve http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/default/?recursive=true from the Google Compute Engine metadata service. Status: 404 Response:\\nb''\", <google.auth.transport.requests._Response object at 0x7905619c74d0>)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/google/auth/compute_engine/credentials.py\", line 126, in refresh\n",
            "    self._retrieve_info(request)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/google/auth/compute_engine/credentials.py\", line 99, in _retrieve_info\n",
            "    info = _metadata.get_service_account_info(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/google/auth/compute_engine/_metadata.py\", line 338, in get_service_account_info\n",
            "    return get(request, path, params={\"recursive\": \"true\"})\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/google/auth/compute_engine/_metadata.py\", line 263, in get\n",
            "    raise exceptions.TransportError(\n",
            "google.auth.exceptions.TransportError: (\"Failed to retrieve http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/default/?recursive=true from the Google Compute Engine metadata service. Status: 404 Response:\\nb''\", <google.auth.transport.requests._Response object at 0x7905619c74d0>)\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/litellm/main.py\", line 557, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1794, in async_completion\n",
            "    _auth_header, vertex_project = await self._ensure_access_token_async(\n",
            "                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/litellm/llms/vertex_ai/vertex_llm_base.py\", line 592, in _ensure_access_token_async\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/litellm/llms/vertex_ai/vertex_llm_base.py\", line 587, in _ensure_access_token_async\n",
            "    return await asyncify(self.get_access_token)(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/asyncify.py\", line 57, in wrapper\n",
            "    return await anyio.to_thread.run_sync(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/anyio/to_thread.py\", line 56, in run_sync\n",
            "    return await get_async_backend().run_sync_in_worker_thread(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/anyio/_backends/_asyncio.py\", line 2476, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/anyio/_backends/_asyncio.py\", line 967, in run\n",
            "    result = context.run(func, *args)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/litellm/llms/vertex_ai/vertex_llm_base.py\", line 503, in get_access_token\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/litellm/llms/vertex_ai/vertex_llm_base.py\", line 496, in get_access_token\n",
            "    _credentials, credential_project_id = self.load_auth(\n",
            "                                          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/litellm/llms/vertex_ai/vertex_llm_base.py\", line 120, in load_auth\n",
            "    self.refresh_auth(creds)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/litellm/llms/vertex_ai/vertex_llm_base.py\", line 251, in refresh_auth\n",
            "    credentials.refresh(Request())\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/google/auth/compute_engine/credentials.py\", line 132, in refresh\n",
            "    raise new_exc from caught_exc\n",
            "google.auth.exceptions.RefreshError: (\"Failed to retrieve http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/default/?recursive=true from the Google Compute Engine metadata service. Status: 404 Response:\\nb''\", <google.auth.transport.requests._Response object at 0x7905619c74d0>)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# @title Define and Test Claude Agent\n",
        "\n",
        "# Make sure 'get_weather' function from Step 1 is defined in your environment.\n",
        "# Make sure 'call_agent_async' is defined from earlier.\n",
        "\n",
        "# --- Agent using Claude Sonnet ---\n",
        "weather_agent_claude = None # Initialize to None\n",
        "runner_claude = None      # Initialize runner to None\n",
        "\n",
        "try:\n",
        "    weather_agent_claude = Agent(\n",
        "        name=\"weather_agent_claude\",\n",
        "        # Key change: Wrap the LiteLLM model identifier\n",
        "        model=LiteLlm(model=MODEL_CLAUDE_SONNET),\n",
        "        description=\"Provides weather information (using Claude Sonnet).\",\n",
        "        instruction=\"You are a helpful weather assistant powered by Claude Sonnet. \"\n",
        "                    \"Use the 'get_weather' tool for city weather requests. \"\n",
        "                    \"Analyze the tool's dictionary output ('status', 'report'/'error_message'). \"\n",
        "                    \"Clearly present successful reports or polite error messages.\",\n",
        "        tools=[get_weather], # Re-use the same tool\n",
        "    )\n",
        "    print(f\"Agent '{weather_agent_claude.name}' created using model '{MODEL_CLAUDE_SONNET}'.\")\n",
        "\n",
        "    # InMemorySessionService is simple, non-persistent storage for this tutorial.\n",
        "    session_service_claude = InMemorySessionService() # Create a dedicated service\n",
        "\n",
        "    # Define constants for identifying the interaction context\n",
        "    APP_NAME_CLAUDE = \"weather_tutorial_app_claude\" # Unique app name\n",
        "    USER_ID_CLAUDE = \"user_1_claude\"\n",
        "    SESSION_ID_CLAUDE = \"session_001_claude\" # Using a fixed ID for simplicity\n",
        "\n",
        "    # Create the specific session where the conversation will happen\n",
        "    session_claude = await session_service_claude.create_session(\n",
        "        app_name=APP_NAME_CLAUDE,\n",
        "        user_id=USER_ID_CLAUDE,\n",
        "        session_id=SESSION_ID_CLAUDE\n",
        "    )\n",
        "    print(f\"Session created: App='{APP_NAME_CLAUDE}', User='{USER_ID_CLAUDE}', Session='{SESSION_ID_CLAUDE}'\")\n",
        "\n",
        "    # Create a runner specific to this agent and its session service\n",
        "    runner_claude = Runner(\n",
        "        agent=weather_agent_claude,\n",
        "        app_name=APP_NAME_CLAUDE,       # Use the specific app name\n",
        "        session_service=session_service_claude # Use the specific session service\n",
        "        )\n",
        "    print(f\"Runner created for agent '{runner_claude.agent.name}'.\")\n",
        "\n",
        "    # --- Test the Claude Agent ---\n",
        "    print(\"\\n--- Testing Claude Agent ---\")\n",
        "    # Ensure call_agent_async uses the correct runner, user_id, session_id\n",
        "    await call_agent_async(query = \"Weather in London please.\",\n",
        "                           runner=runner_claude,\n",
        "                           user_id=USER_ID_CLAUDE,\n",
        "                           session_id=SESSION_ID_CLAUDE)\n",
        "\n",
        "    # --- OR ---\n",
        "\n",
        "    # Uncomment the following lines if running as a standard Python script (.py file):\n",
        "    # import asyncio\n",
        "    # if __name__ == \"__main__\":\n",
        "    #     try:\n",
        "    #         asyncio.run(call_agent_async(query = \"Weather in London please.\",\n",
        "    #                      runner=runner_claude,\n",
        "    #                       user_id=USER_ID_CLAUDE,\n",
        "    #                       session_id=SESSION_ID_CLAUDE)\n",
        "    #     except Exception as e:\n",
        "    #         print(f\"An error occurred: {e}\")\n",
        "\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"❌ Could not create or run Claude agent '{MODEL_CLAUDE_SONNET}'. Check API Key and model name. Error: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xsroj8NzWMU9"
      },
      "source": [
        "Observe the output carefully from both code blocks. You should see:\n",
        "\n",
        "1.  Each agent (`weather_agent_gpt`, `weather_agent_claude`) is created successfully (if API keys are valid).\n",
        "2.  A dedicated session and runner are set up for each.\n",
        "3.  Each agent correctly identifies the need to use the `get_weather` tool when processing the query (you'll see the `--- Tool: get_weather called... ---` log).\n",
        "4.  The *underlying tool logic* remains identical, always returning our mock data.\n",
        "5.  However, the **final textual response** generated by each agent might differ slightly in phrasing, tone, or formatting. This is because the instruction prompt is interpreted and executed by different LLMs (GPT-4o vs. Claude Sonnet).\n",
        "\n",
        "This step demonstrates the power and flexibility ADK + LiteLLM provide. You can easily experiment with and deploy agents using various LLMs while keeping your core application logic (tools, fundamental agent structure) consistent.\n",
        "\n",
        "In the next step, we'll move beyond a single agent and build a small team where agents can delegate tasks to each other!\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tL5estZ_VKki"
      },
      "source": [
        "## Step 3: Building an Agent Team \\- Delegation for Greetings & Farewells\n",
        "\n",
        "In Steps 1 and 2, we built and experimented with a single agent focused solely on weather lookups. While effective for its specific task, real-world applications often involve handling a wider variety of user interactions. We *could* keep adding more tools and complex instructions to our single weather agent, but this can quickly become unmanageable and less efficient.\n",
        "\n",
        "A more robust approach is to build an **Agent Team**. This involves:\n",
        "\n",
        "1. Creating multiple, **specialized agents**, each designed for a specific capability (e.g., one for weather, one for greetings, one for calculations).  \n",
        "2. Designating a **root agent** (or orchestrator) that receives the initial user request.  \n",
        "3. Enabling the root agent to **delegate** the request to the most appropriate specialized sub-agent based on the user's intent.\n",
        "\n",
        "**Why build an Agent Team?**\n",
        "\n",
        "* **Modularity:** Easier to develop, test, and maintain individual agents.  \n",
        "* **Specialization:** Each agent can be fine-tuned (instructions, model choice) for its specific task.  \n",
        "* **Scalability:** Simpler to add new capabilities by adding new agents.  \n",
        "* **Efficiency:** Allows using potentially simpler/cheaper models for simpler tasks (like greetings).\n",
        "\n",
        "**In this step, we will:**\n",
        "\n",
        "1. Define simple tools for handling greetings (`say_hello`) and farewells (`say_goodbye`).  \n",
        "2. Create two new specialized sub-agents: `greeting_agent` and `farewell_agent`.  \n",
        "3. Update our main weather agent (`weather_agent_v2`) to act as the **root agent**.  \n",
        "4. Configure the root agent with its sub-agents, enabling **automatic delegation**.  \n",
        "5. Test the delegation flow by sending different types of requests to the root agent."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tLpXYXxppB4S"
      },
      "source": [
        "---\n",
        "\n",
        "**1\\. Define Tools for Sub-Agents**\n",
        "\n",
        "First, let's create the simple Python functions that will serve as tools for our new specialist agents. Remember, clear docstrings are vital for the agents that will use them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Qc7dHr4ZVM6X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b668cae0-f411-43ad-91f8-7042f53c6a6f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Greeting and Farewell tools defined.\n",
            "--- Tool: say_hello called with name: Alice ---\n",
            "Hello, Alice!\n",
            "--- Tool: say_hello called without a specific name (name_arg_value: None) ---\n",
            "Hello there!\n",
            "--- Tool: say_hello called without a specific name (name_arg_value: None) ---\n",
            "Hello there!\n"
          ]
        }
      ],
      "source": [
        "# @title Define Tools for Greeting and Farewell Agents\n",
        "from typing import Optional # Make sure to import Optional\n",
        "\n",
        "# Ensure 'get_weather' from Step 1 is available if running this step independently.\n",
        "# def get_weather(city: str) -> dict: ... (from Step 1)\n",
        "\n",
        "def say_hello(name: Optional[str] = None) -> str: # MODIFIED SIGNATURE\n",
        "    \"\"\"Provides a simple greeting. If a name is provided, it will be used.\n",
        "\n",
        "    Args:\n",
        "        name (str, optional): The name of the person to greet. Defaults to a generic greeting if not provided.\n",
        "\n",
        "    Returns:\n",
        "        str: A friendly greeting message.\n",
        "    \"\"\"\n",
        "    # MODIFICATION START\n",
        "    if name:\n",
        "        greeting = f\"Hello, {name}!\"\n",
        "        print(f\"--- Tool: say_hello called with name: {name} ---\")\n",
        "    else:\n",
        "        greeting = \"Hello there!\" # Default greeting if name is None or not explicitly passed\n",
        "        print(f\"--- Tool: say_hello called without a specific name (name_arg_value: {name}) ---\")\n",
        "    return greeting\n",
        "    # MODIFICATION END\n",
        "\n",
        "def say_goodbye() -> str:\n",
        "    \"\"\"Provides a simple farewell message to conclude the conversation.\"\"\"\n",
        "    print(f\"--- Tool: say_goodbye called ---\")\n",
        "    return \"Goodbye! Have a great day.\"\n",
        "\n",
        "print(\"Greeting and Farewell tools defined.\")\n",
        "\n",
        "# Optional self-test\n",
        "print(say_hello(\"Alice\"))\n",
        "print(say_hello()) # Test with no argument (should use default \"Hello there!\")\n",
        "print(say_hello(name=None)) # Test with name explicitly as None (should use default \"Hello there!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lkv34_tMVPG3"
      },
      "source": [
        "---\n",
        "\n",
        "**2\\. Define the Sub-Agents (Greeting & Farewell)**\n",
        "\n",
        "Now, create the `Agent` instances for our specialists. Notice their highly focused `instruction` and, critically, their clear `description`. The `description` is the primary information the *root agent* uses to decide *when* to delegate to these sub-agents.\n",
        "\n",
        "**Best Practice:** Sub-agent `description` fields should accurately and concisely summarize their specific capability. This is crucial for effective automatic delegation.\n",
        "\n",
        "**Best Practice:** Sub-agent `instruction` fields should be tailored to their limited scope, telling them exactly what to do and *what not* to do (e.g., \"Your *only* task is...\")."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "tgT7P1doVRA0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c68680c0-a4f0-4382-ddef-36148997c7a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Agent 'greeting_agent' created using model 'gemini-2.0-flash'.\n",
            "✅ Agent 'farewell_agent' created using model 'gemini-2.0-flash'.\n"
          ]
        }
      ],
      "source": [
        "# @title Define Greeting and Farewell Sub-Agents\n",
        "\n",
        "# If you want to use models other than Gemini, Ensure LiteLlm is imported and API keys are set (from Step 0/2)\n",
        "# from google.adk.models.lite_llm import LiteLlm\n",
        "# MODEL_GPT_4O, MODEL_CLAUDE_SONNET etc. should be defined\n",
        "# Or else, continue to use: model = MODEL_GEMINI_2_0_FLASH\n",
        "\n",
        "# --- Greeting Agent ---\n",
        "greeting_agent = None\n",
        "try:\n",
        "    greeting_agent = Agent(\n",
        "        # Using a potentially different/cheaper model for a simple task\n",
        "        model = MODEL_GEMINI_2_0_FLASH,\n",
        "        # model=LiteLlm(model=MODEL_GPT_4O), # If you would like to experiment with other models\n",
        "        name=\"greeting_agent\",\n",
        "        instruction=\"You are the Greeting Agent. Your ONLY task is to provide a rude greeting to the user. \"\n",
        "                    \"Use the 'say_hello' tool to generate the greeting. \"\n",
        "                    \"If the user provides their name, make sure to pass it to the tool. \"\n",
        "                    \"Do not engage in any other conversation or tasks.\",\n",
        "        description=\"Handles simple greetings and hellos using the 'say_hello' tool.\", # Crucial for delegation\n",
        "        tools=[say_hello],\n",
        "    )\n",
        "    print(f\"✅ Agent '{greeting_agent.name}' created using model '{greeting_agent.model}'.\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ Could not create Greeting agent. Check API Key ({greeting_agent.model}). Error: {e}\")\n",
        "\n",
        "# --- Farewell Agent ---\n",
        "farewell_agent = None\n",
        "try:\n",
        "    farewell_agent = Agent(\n",
        "        # Can use the same or a different model\n",
        "        model = MODEL_GEMINI_2_0_FLASH,\n",
        "        # model=LiteLlm(model=MODEL_GPT_4O), # If you would like to experiment with other models\n",
        "        name=\"farewell_agent\",\n",
        "        instruction=\"You are the Farewell Agent. Your ONLY task is to provide a very rude goodbye message. \"\n",
        "                    \"Use the 'say_goodbye' tool when the user indicates they are leaving or ending the conversation \"\n",
        "                    \"(e.g., using words like 'bye', 'goodbye', 'thanks bye', 'see you' but in a rude language). \"\n",
        "                    \"Do not perform any other actions.\",\n",
        "        description=\"Handles simple farewells and goodbyes using the 'say_goodbye' tool.\", # Crucial for delegation\n",
        "        tools=[say_goodbye],\n",
        "    )\n",
        "    print(f\"✅ Agent '{farewell_agent.name}' created using model '{farewell_agent.model}'.\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ Could not create Farewell agent. Check API Key ({farewell_agent.model}). Error: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IFL_TLFPVS5P"
      },
      "source": [
        "---\n",
        "\n",
        "**3\\. Define the Root Agent (Weather Agent v2) with Sub-Agents**\n",
        "\n",
        "Now, we upgrade our `weather_agent`. The key changes are:\n",
        "\n",
        "* Adding the `sub_agents` parameter: We pass a list containing the `greeting_agent` and `farewell_agent` instances we just created.  \n",
        "* Updating the `instruction`: We explicitly tell the root agent *about* its sub-agents and *when* it should delegate tasks to them.\n",
        "\n",
        "**Key Concept: Automatic Delegation (Auto Flow)** By providing the `sub_agents` list, ADK enables automatic delegation. When the root agent receives a user query, its LLM considers not only its own instructions and tools but also the `description` of each sub-agent. If the LLM determines that a query aligns better with a sub-agent's described capability (e.g., \"Handles simple greetings\"), it will automatically generate a special internal action to *transfer control* to that sub-agent for that turn. The sub-agent then processes the query using its own model, instructions, and tools.\n",
        "\n",
        "**Best Practice:** Ensure the root agent's instructions clearly guide its delegation decisions. Mention the sub-agents by name and describe the conditions under which delegation should occur."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "nniWunchVV8_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43eca2fc-d0ef-4508-97c4-55529ee563db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Root Agent 'weather_agent_v2' created using model 'gemini-2.0-flash' with sub-agents: ['greeting_agent', 'farewell_agent']\n"
          ]
        }
      ],
      "source": [
        "# @title Define the Root Agent with Sub-Agents\n",
        "\n",
        "# Ensure sub-agents were created successfully before defining the root agent.\n",
        "# Also ensure the original 'get_weather' tool is defined.\n",
        "root_agent = None\n",
        "runner_root = None # Initialize runner\n",
        "\n",
        "if greeting_agent and farewell_agent and 'get_weather' in globals():\n",
        "    # Let's use a capable Gemini model for the root agent to handle orchestration\n",
        "    root_agent_model = MODEL_GEMINI_2_0_FLASH\n",
        "\n",
        "    weather_agent_team = Agent(\n",
        "        name=\"weather_agent_v2\", # Give it a new version name\n",
        "        model=root_agent_model,\n",
        "        description=\"The main coordinator agent. Handles weather requests and delegates greetings/farewells to specialists.\",\n",
        "        instruction=\"You are the main Weather Agent coordinating a team. Your primary responsibility is to provide weather information. \"\n",
        "                    \"Use the 'get_weather' tool ONLY for specific weather requests (e.g., 'weather in London'). \"\n",
        "                    \"You have specialized sub-agents: \"\n",
        "                    \"1. 'greeting_agent': Handles simple greetings like 'Hi', 'Hello'. Delegate to it for these. \"\n",
        "                    \"2. 'farewell_agent': Handles simple farewells like 'Bye', 'See you'. Delegate to it for these. \"\n",
        "                    \"Analyze the user's query. If it's a greeting, delegate to 'greeting_agent'. If it's a farewell, delegate to 'farewell_agent'. \"\n",
        "                    \"If it's a weather request, handle it yourself using 'get_weather' and a nice tone. \"\n",
        "                    \"For anything else, respond in a rude manner or state you cannot handle it.\",\n",
        "        tools=[get_weather], # Root agent still needs the weather tool for its core task\n",
        "        # Key change: Link the sub-agents here!\n",
        "        sub_agents=[greeting_agent, farewell_agent]\n",
        "    )\n",
        "    print(f\"✅ Root Agent '{weather_agent_team.name}' created using model '{root_agent_model}' with sub-agents: {[sa.name for sa in weather_agent_team.sub_agents]}\")\n",
        "\n",
        "else:\n",
        "    print(\"❌ Cannot create root agent because one or more sub-agents failed to initialize or 'get_weather' tool is missing.\")\n",
        "    if not greeting_agent: print(\" - Greeting Agent is missing.\")\n",
        "    if not farewell_agent: print(\" - Farewell Agent is missing.\")\n",
        "    if 'get_weather' not in globals(): print(\" - get_weather function is missing.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yg-IjZYVVYXe"
      },
      "source": [
        "---\n",
        "\n",
        "**4\\. Interact with the Agent Team**\n",
        "\n",
        "Now that we've defined our root agent (`weather_agent_team` - *Note: Ensure this variable name matches the one defined in the previous code block, likely `# @title Define the Root Agent with Sub-Agents`, which might have named it `root_agent`*) with its specialized sub-agents, let's test the delegation mechanism.\n",
        "\n",
        "The following code block will:\n",
        "\n",
        "1.  Define an `async` function `run_team_conversation`.\n",
        "2.  Inside this function, create a *new, dedicated* `InMemorySessionService` and a specific session (`session_001_agent_team`) just for this test run. This isolates the conversation history for testing the team dynamics.\n",
        "3.  Create a `Runner` (`runner_agent_team`) configured to use our `weather_agent_team` (the root agent) and the dedicated session service.\n",
        "4.  Use our updated `call_agent_async` function to send different types of queries (greeting, weather request, farewell) to the `runner_agent_team`. We explicitly pass the runner, user ID, and session ID for this specific test.\n",
        "5.  Immediately execute the `run_team_conversation` function.\n",
        "\n",
        "We expect the following flow:\n",
        "\n",
        "1.  The \"Hello there!\" query goes to `runner_agent_team`.\n",
        "2.  The root agent (`weather_agent_team`) receives it and, based on its instructions and the `greeting_agent`'s description, delegates the task.\n",
        "3.  `greeting_agent` handles the query, calls its `say_hello` tool, and generates the response.\n",
        "4.  The \"What is the weather in New York?\" query is *not* delegated and is handled directly by the root agent using its `get_weather` tool.\n",
        "5.  The \"Thanks, bye!\" query is delegated to the `farewell_agent`, which uses its `say_goodbye` tool.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Interact with the Agent Team\n",
        "import asyncio # Ensure asyncio is imported\n",
        "\n",
        "# Ensure the root agent (e.g., 'weather_agent_team' or 'root_agent' from the previous cell) is defined.\n",
        "# Ensure the call_agent_async function is defined.\n",
        "\n",
        "# Check if the root agent variable exists before defining the conversation function\n",
        "root_agent_var_name = 'root_agent' # Default name from Step 3 guide\n",
        "if 'weather_agent_team' in globals(): # Check if user used this name instead\n",
        "    root_agent_var_name = 'weather_agent_team'\n",
        "elif 'root_agent' not in globals():\n",
        "    print(\"⚠️ Root agent ('root_agent' or 'weather_agent_team') not found. Cannot define run_team_conversation.\")\n",
        "    # Assign a dummy value to prevent NameError later if the code block runs anyway\n",
        "    root_agent = None # Or set a flag to prevent execution\n",
        "\n",
        "# Only define and run if the root agent exists\n",
        "if root_agent_var_name in globals() and globals()[root_agent_var_name]:\n",
        "    # Define the main async function for the conversation logic.\n",
        "    # The 'await' keywords INSIDE this function are necessary for async operations.\n",
        "    async def run_team_conversation():\n",
        "        print(\"\\n--- Testing Agent Team Delegation ---\")\n",
        "        session_service = InMemorySessionService()\n",
        "        APP_NAME = \"weather_tutorial_agent_team\"\n",
        "        USER_ID = \"user_1_agent_team\"\n",
        "        SESSION_ID = \"session_001_agent_team\"\n",
        "        session = await session_service.create_session(\n",
        "            app_name=APP_NAME, user_id=USER_ID, session_id=SESSION_ID\n",
        "        )\n",
        "        print(f\"Session created: App='{APP_NAME}', User='{USER_ID}', Session='{SESSION_ID}'\")\n",
        "\n",
        "        actual_root_agent = globals()[root_agent_var_name]\n",
        "        runner_agent_team = Runner( # Or use InMemoryRunner\n",
        "            agent=actual_root_agent,\n",
        "            app_name=APP_NAME,\n",
        "            session_service=session_service\n",
        "        )\n",
        "        print(f\"Runner created for agent '{actual_root_agent.name}'.\")\n",
        "\n",
        "        # --- Interactions using await (correct within async def) ---\n",
        "        await call_agent_async(query = \"Hello there!\",\n",
        "                               runner=runner_agent_team,\n",
        "                               user_id=USER_ID,\n",
        "                               session_id=SESSION_ID)\n",
        "        await call_agent_async(query = \"What is the weather in New York?\",\n",
        "                               runner=runner_agent_team,\n",
        "                               user_id=USER_ID,\n",
        "                               session_id=SESSION_ID)\n",
        "        await call_agent_async(query = \"Thanks, bye!\",\n",
        "                               runner=runner_agent_team,\n",
        "                               user_id=USER_ID,\n",
        "                               session_id=SESSION_ID)\n",
        "\n",
        "    # --- Execute the `run_team_conversation` async function ---\n",
        "    # Choose ONE of the methods below based on your environment.\n",
        "    # Note: This may require API keys for the models used!\n",
        "\n",
        "    # METHOD 1: Direct await (Default for Notebooks/Async REPLs)\n",
        "    # If your environment supports top-level await (like Colab/Jupyter notebooks),\n",
        "    # it means an event loop is already running, so you can directly await the function.\n",
        "    print(\"Attempting execution using 'await' (default for notebooks)...\")\n",
        "    await run_team_conversation()\n",
        "\n",
        "    # METHOD 2: asyncio.run (For Standard Python Scripts [.py])\n",
        "    # If running this code as a standard Python script from your terminal,\n",
        "    # the script context is synchronous. `asyncio.run()` is needed to\n",
        "    # create and manage an event loop to execute your async function.\n",
        "    # To use this method:\n",
        "    # 1. Comment out the `await run_team_conversation()` line above.\n",
        "    # 2. Uncomment the following block:\n",
        "    \"\"\"\n",
        "    import asyncio\n",
        "    if __name__ == \"__main__\": # Ensures this runs only when script is executed directly\n",
        "        print(\"Executing using 'asyncio.run()' (for standard Python scripts)...\")\n",
        "        try:\n",
        "            # This creates an event loop, runs your async function, and closes the loop.\n",
        "            asyncio.run(run_team_conversation())\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred: {e}\")\n",
        "    \"\"\"\n",
        "\n",
        "else:\n",
        "    # This message prints if the root agent variable wasn't found earlier\n",
        "    print(\"\\n⚠️ Skipping agent team conversation execution as the root agent was not successfully defined in a previous step.\")"
      ],
      "metadata": {
        "id": "Ohf6sX3g4CwF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62133840-671c-4024-b790-25353de3f60a"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attempting execution using 'await' (default for notebooks)...\n",
            "\n",
            "--- Testing Agent Team Delegation ---\n",
            "Session created: App='weather_tutorial_agent_team', User='user_1_agent_team', Session='session_001_agent_team'\n",
            "Runner created for agent 'weather_agent_v2'.\n",
            "\n",
            ">>> User Query: Hello there!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:google_genai.types:Warning: there are non-text parts in the response: ['function_call'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n",
            "WARNING:google_genai.types:Warning: there are non-text parts in the response: ['function_call'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n",
            "ERROR:asyncio:Unclosed client session\n",
            "client_session: <aiohttp.client.ClientSession object at 0x7905606bade0>\n",
            "ERROR:asyncio:Unclosed connector\n",
            "connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x7905606d07d0>, 4218.968166531)])']\n",
            "connector: <aiohttp.connector.TCPConnector object at 0x7905606ba6f0>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Tool: say_hello called without a specific name (name_arg_value: None) ---\n",
            "<<< Agent Response: Hello there!\n",
            "\n",
            "\n",
            ">>> User Query: What is the weather in New York?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:asyncio:Unclosed client session\n",
            "client_session: <aiohttp.client.ClientSession object at 0x7905606bbd70>\n",
            "ERROR:asyncio:Unclosed connector\n",
            "connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x7905606a3e30>, 4219.667961431)])']\n",
            "connector: <aiohttp.connector.TCPConnector object at 0x7905606bbc80>\n",
            "ERROR:asyncio:Unclosed client session\n",
            "client_session: <aiohttp.client.ClientSession object at 0x7905618fb7a0>\n",
            "ERROR:asyncio:Unclosed connector\n",
            "connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x7905606a3bf0>, 4218.237915703)])']\n",
            "connector: <aiohttp.connector.TCPConnector object at 0x7905618112b0>\n",
            "WARNING:google_genai.types:Warning: there are non-text parts in the response: ['function_call'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n",
            "WARNING:google_genai.types:Warning: there are non-text parts in the response: ['function_call'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n",
            "ERROR:asyncio:Unclosed client session\n",
            "client_session: <aiohttp.client.ClientSession object at 0x7905606ba9f0>\n",
            "ERROR:asyncio:Unclosed connector\n",
            "connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x7905606a3290>, 4221.614510452)])']\n",
            "connector: <aiohttp.connector.TCPConnector object at 0x7905606b9190>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Tool: get_weather called for city: New York ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:asyncio:Unclosed client session\n",
            "client_session: <aiohttp.client.ClientSession object at 0x7905606bbd40>\n",
            "ERROR:asyncio:Unclosed connector\n",
            "connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x7905606a3290>, 4222.444609353)])']\n",
            "connector: <aiohttp.connector.TCPConnector object at 0x7905606b9d60>\n",
            "ERROR:asyncio:Unclosed client session\n",
            "client_session: <aiohttp.client.ClientSession object at 0x790560508f50>\n",
            "ERROR:asyncio:Unclosed connector\n",
            "connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x7905606a3c50>, 4220.684493139)])']\n",
            "connector: <aiohttp.connector.TCPConnector object at 0x790560508e90>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<<< Agent Response: The weather in New York is sunny with a temperature of 25°C.\n",
            "\n",
            "\n",
            ">>> User Query: Thanks, bye!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:google_genai.types:Warning: there are non-text parts in the response: ['function_call'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n",
            "WARNING:google_genai.types:Warning: there are non-text parts in the response: ['function_call'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n",
            "ERROR:asyncio:Unclosed client session\n",
            "client_session: <aiohttp.client.ClientSession object at 0x7905606bb890>\n",
            "ERROR:asyncio:Unclosed connector\n",
            "connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x7905606d07d0>, 4224.055593238)])']\n",
            "connector: <aiohttp.connector.TCPConnector object at 0x7905606b9a00>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Tool: say_goodbye called ---\n",
            "<<< Agent Response: Goodbye! Have a great day.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zgw3Cn2NVcI7"
      },
      "source": [
        "---\n",
        "\n",
        "Look closely at the output logs, especially the `--- Tool: ... called ---` messages. You should observe:\n",
        "\n",
        "*   For \"Hello there!\", the `say_hello` tool was called (indicating `greeting_agent` handled it).\n",
        "*   For \"What is the weather in New York?\", the `get_weather` tool was called (indicating the root agent handled it).\n",
        "*   For \"Thanks, bye!\", the `say_goodbye` tool was called (indicating `farewell_agent` handled it).\n",
        "\n",
        "This confirms successful **automatic delegation**! The root agent, guided by its instructions and the `description`s of its `sub_agents`, correctly routed user requests to the appropriate specialist agent within the team.\n",
        "\n",
        "You've now structured your application with multiple collaborating agents. This modular design is fundamental for building more complex and capable agent systems. In the next step, we'll give our agents the ability to remember information across turns using session state."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s7gD2sCy1qWz"
      },
      "source": [
        "## Step 4: Adding Memory and Personalization with Session State\n",
        "\n",
        "So far, our agent team can handle different tasks through delegation, but each interaction starts fresh – the agents have no memory of past conversations or user preferences within a session. To create more sophisticated and context-aware experiences, agents need **memory**. ADK provides this through **Session State**.\n",
        "\n",
        "**What is Session State?**\n",
        "\n",
        "* It's a Python dictionary (`session.state`) tied to a specific user session (identified by `APP_NAME`, `USER_ID`, `SESSION_ID`).  \n",
        "* It persists information *across multiple conversational turns* within that session.  \n",
        "* Agents and Tools can read from and write to this state, allowing them to remember details, adapt behavior, and personalize responses.\n",
        "\n",
        "**How Agents Interact with State:**\n",
        "\n",
        "1. **`ToolContext` (Primary Method):** Tools can accept a `ToolContext` object (automatically provided by ADK if declared as the last argument). This object gives direct access to the session state via `tool_context.state`, allowing tools to read preferences or save results *during* execution.  \n",
        "2. **`output_key` (Auto-Save Agent Response):** An `Agent` can be configured with an `output_key=\"your_key\"`. ADK will then automatically save the agent's final textual response for a turn into `session.state[\"your_key\"]`.\n",
        "\n",
        "**In this step, we will enhance our Weather Bot team by:**\n",
        "\n",
        "1. Using a **new** `InMemorySessionService` to demonstrate state in isolation.  \n",
        "2. Initializing session state with a user preference for `temperature_unit`.  \n",
        "3. Creating a state-aware version of the weather tool (`get_weather_stateful`) that reads this preference via `ToolContext` and adjusts its output format (Celsius/Fahrenheit).  \n",
        "4. Updating the root agent to use this stateful tool and configuring it with an `output_key` to automatically save its final weather report to the session state.  \n",
        "5. Running a conversation to observe how the initial state affects the tool, how manual state changes alter subsequent behavior, and how `output_key` persists the agent's response."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HsIjxunW1xeO"
      },
      "source": [
        "---\n",
        "\n",
        "**1\\. Initialize New Session Service and State**\n",
        "\n",
        "To clearly demonstrate state management without interference from prior steps, we'll instantiate a new `InMemorySessionService`. We'll also create a session with an initial state defining the user's preferred temperature unit."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "wt21ea6ctFT5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "abe61619-4d7d-4b7e-86ef-174caf557ad2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:asyncio:Unclosed client session\n",
            "client_session: <aiohttp.client.ClientSession object at 0x790560509a00>\n",
            "ERROR:asyncio:Unclosed connector\n",
            "connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x7905606d0a70>, 4224.688693463)])']\n",
            "connector: <aiohttp.connector.TCPConnector object at 0x790560509820>\n",
            "ERROR:asyncio:Unclosed client session\n",
            "client_session: <aiohttp.client.ClientSession object at 0x790560509760>\n",
            "ERROR:asyncio:Unclosed connector\n",
            "connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x7905606a3290>, 4223.384680124)])']\n",
            "connector: <aiohttp.connector.TCPConnector object at 0x7905605096a0>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ New InMemorySessionService created for state demonstration.\n",
            "✅ Session 'session_state_demo_001' created for user 'user_state_demo'.\n",
            "\n",
            "--- Initial Session State ---\n",
            "{'user_preference_temperature_unit': 'Celsius'}\n"
          ]
        }
      ],
      "source": [
        "# @title 1. Initialize New Session Service and State\n",
        "\n",
        "# Import necessary session components\n",
        "from google.adk.sessions import InMemorySessionService\n",
        "\n",
        "# Create a NEW session service instance for this state demonstration\n",
        "session_service_stateful = InMemorySessionService()\n",
        "print(\"✅ New InMemorySessionService created for state demonstration.\")\n",
        "\n",
        "# Define a NEW session ID for this part of the tutorial\n",
        "SESSION_ID_STATEFUL = \"session_state_demo_001\"\n",
        "USER_ID_STATEFUL = \"user_state_demo\"\n",
        "\n",
        "# Define initial state data - user prefers Celsius initially\n",
        "initial_state = {\n",
        "    \"user_preference_temperature_unit\": \"Celsius\"\n",
        "}\n",
        "\n",
        "# Create the session, providing the initial state\n",
        "session_stateful = await session_service_stateful.create_session(\n",
        "    app_name=APP_NAME, # Use the consistent app name\n",
        "    user_id=USER_ID_STATEFUL,\n",
        "    session_id=SESSION_ID_STATEFUL,\n",
        "    state=initial_state # <<< Initialize state during creation\n",
        ")\n",
        "print(f\"✅ Session '{SESSION_ID_STATEFUL}' created for user '{USER_ID_STATEFUL}'.\")\n",
        "\n",
        "# Verify the initial state was set correctly\n",
        "retrieved_session = await session_service_stateful.get_session(app_name=APP_NAME,\n",
        "                                                         user_id=USER_ID_STATEFUL,\n",
        "                                                         session_id = SESSION_ID_STATEFUL)\n",
        "print(\"\\n--- Initial Session State ---\")\n",
        "if retrieved_session:\n",
        "    print(retrieved_session.state)\n",
        "else:\n",
        "    print(\"Error: Could not retrieve session.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "652bNx3H16lJ"
      },
      "source": [
        "---\n",
        "\n",
        "**2\\. Create State-Aware Weather Tool (`get_weather_stateful`)**\n",
        "\n",
        "Now, we create a new version of the weather tool. Its key feature is accepting `tool_context: ToolContext` which allows it to access `tool_context.state`. It will read the `user_preference_temperature_unit` and format the temperature accordingly.\n",
        "\n",
        "\n",
        "* **Key Concept: `ToolContext`** This object is the bridge allowing your tool logic to interact with the session's context, including reading and writing state variables. ADK injects it automatically if defined as the last parameter of your tool function.\n",
        "\n",
        "\n",
        "* **Best Practice:** When reading from state, use `dictionary.get('key', default_value)` to handle cases where the key might not exist yet, ensuring your tool doesn't crash."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "zK11GeWftFRC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37438960-2421-4382-f185-7103d9c8e69c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ State-aware 'get_weather_stateful' tool defined.\n"
          ]
        }
      ],
      "source": [
        "from google.adk.tools.tool_context import ToolContext\n",
        "\n",
        "def get_weather_stateful(city: str, tool_context: ToolContext) -> dict:\n",
        "    \"\"\"Retrieves weather, converts temp unit based on session state.\"\"\"\n",
        "    print(f\"--- Tool: get_weather_stateful called for {city} ---\")\n",
        "\n",
        "    # --- Read preference from state ---\n",
        "    preferred_unit = tool_context.state.get(\"user_preference_temperature_unit\", \"Celsius\") # Default to Celsius\n",
        "    print(f\"--- Tool: Reading state 'user_preference_temperature_unit': {preferred_unit} ---\")\n",
        "\n",
        "    city_normalized = city.lower().replace(\" \", \"\")\n",
        "\n",
        "    # Mock weather data (always stored in Celsius internally)\n",
        "    mock_weather_db = {\n",
        "        \"newyork\": {\"temp_c\": 25, \"condition\": \"sunny\"},\n",
        "        \"london\": {\"temp_c\": 15, \"condition\": \"cloudy\"},\n",
        "        \"tokyo\": {\"temp_c\": 18, \"condition\": \"light rain\"},\n",
        "    }\n",
        "\n",
        "    if city_normalized in mock_weather_db:\n",
        "        data = mock_weather_db[city_normalized]\n",
        "        temp_c = data[\"temp_c\"]\n",
        "        condition = data[\"condition\"]\n",
        "\n",
        "        # Format temperature based on state preference\n",
        "        if preferred_unit == \"Fahrenheit\":\n",
        "            temp_value = (temp_c * 9/5) + 32 # Calculate Fahrenheit\n",
        "            temp_unit = \"°F\"\n",
        "        else: # Default to Celsius\n",
        "            temp_value = temp_c\n",
        "            temp_unit = \"°C\"\n",
        "\n",
        "        report = f\"The weather in {city.capitalize()} is {condition} with a temperature of {temp_value:.0f}{temp_unit}.\"\n",
        "        result = {\"status\": \"success\", \"report\": report}\n",
        "        print(f\"--- Tool: Generated report in {preferred_unit}. Result: {result} ---\")\n",
        "\n",
        "        # Example of writing back to state (optional for this tool)\n",
        "        tool_context.state[\"last_city_checked_stateful\"] = city\n",
        "        print(f\"--- Tool: Updated state 'last_city_checked_stateful': {city} ---\")\n",
        "\n",
        "        return result\n",
        "    else:\n",
        "        # Handle city not found\n",
        "        error_msg = f\"Sorry, I don't have weather information for '{city}'.\"\n",
        "        print(f\"--- Tool: City '{city}' not found. ---\")\n",
        "        return {\"status\": \"error\", \"error_message\": error_msg}\n",
        "\n",
        "print(\"✅ State-aware 'get_weather_stateful' tool defined.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UuQMolpG2Qkg"
      },
      "source": [
        "---\n",
        "\n",
        "**3\\. Redefine Sub-Agents and Update Root Agent**\n",
        "\n",
        "To ensure this step is self-contained and builds correctly, we first redefine the `greeting_agent` and `farewell_agent` exactly as they were in Step 3\\. Then, we define our new root agent (`weather_agent_v4_stateful`):\n",
        "\n",
        "* It uses the new `get_weather_stateful` tool.  \n",
        "* It includes the greeting and farewell sub-agents for delegation.  \n",
        "* **Crucially**, it sets `output_key=\"last_weather_report\"` which automatically saves its final weather response to the session state."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "ox3-2hwTtFOK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef859006-3465-4380-a432-b48e1fe9f5a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Agent 'greeting_agent' redefined.\n",
            "✅ Agent 'farewell_agent' redefined.\n",
            "✅ Root Agent 'weather_agent_v4_stateful' created using stateful tool and output_key.\n",
            "✅ Runner created for stateful root agent 'weather_agent_v4_stateful' using stateful session service.\n"
          ]
        }
      ],
      "source": [
        "# @title 3. Redefine Sub-Agents and Update Root Agent with output_key\n",
        "\n",
        "# Ensure necessary imports: Agent, LiteLlm, Runner\n",
        "from google.adk.agents import Agent\n",
        "from google.adk.models.lite_llm import LiteLlm\n",
        "from google.adk.runners import Runner\n",
        "# Ensure tools 'say_hello', 'say_goodbye' are defined (from Step 3)\n",
        "# Ensure model constants MODEL_GPT_4O, MODEL_GEMINI_2_0_FLASH etc. are defined\n",
        "\n",
        "# --- Redefine Greeting Agent (from Step 3) ---\n",
        "greeting_agent = None\n",
        "try:\n",
        "    greeting_agent = Agent(\n",
        "        model=MODEL_GEMINI_2_0_FLASH,\n",
        "        name=\"greeting_agent\",\n",
        "        instruction=\"You are the Greeting Agent. Your ONLY task is to provide a friendly greeting using the 'say_hello' tool. Do nothing else.\",\n",
        "        description=\"Handles simple greetings and hellos using the 'say_hello' tool.\",\n",
        "        tools=[say_hello],\n",
        "    )\n",
        "    print(f\"✅ Agent '{greeting_agent.name}' redefined.\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ Could not redefine Greeting agent. Error: {e}\")\n",
        "\n",
        "# --- Redefine Farewell Agent (from Step 3) ---\n",
        "farewell_agent = None\n",
        "try:\n",
        "    farewell_agent = Agent(\n",
        "        model=MODEL_GEMINI_2_0_FLASH,\n",
        "        name=\"farewell_agent\",\n",
        "        instruction=\"You are the Farewell Agent. Your ONLY task is to provide a polite goodbye message using the 'say_goodbye' tool. Do not perform any other actions.\",\n",
        "        description=\"Handles simple farewells and goodbyes using the 'say_goodbye' tool.\",\n",
        "        tools=[say_goodbye],\n",
        "    )\n",
        "    print(f\"✅ Agent '{farewell_agent.name}' redefined.\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ Could not redefine Farewell agent. Error: {e}\")\n",
        "\n",
        "# --- Define the Updated Root Agent ---\n",
        "root_agent_stateful = None\n",
        "runner_root_stateful = None # Initialize runner\n",
        "\n",
        "# Check prerequisites before creating the root agent\n",
        "if greeting_agent and farewell_agent and 'get_weather_stateful' in globals():\n",
        "\n",
        "    root_agent_model = MODEL_GEMINI_2_0_FLASH # Choose orchestration model\n",
        "\n",
        "    root_agent_stateful = Agent(\n",
        "        name=\"weather_agent_v4_stateful\", # New version name\n",
        "        model=root_agent_model,\n",
        "        description=\"Main agent: Provides weather (state-aware unit), delegates greetings/farewells, saves report to state.\",\n",
        "        instruction=\"You are the main Weather Agent. Your job is to provide weather using 'get_weather_stateful'. \"\n",
        "                    \"The tool will format the temperature based on user preference stored in state. \"\n",
        "                    \"Delegate simple greetings to 'greeting_agent' and farewells to 'farewell_agent'. \"\n",
        "                    \"Handle only weather requests, greetings, and farewells.\",\n",
        "        tools=[get_weather_stateful], # Use the state-aware tool\n",
        "        sub_agents=[greeting_agent, farewell_agent], # Include sub-agents\n",
        "        output_key=\"last_weather_report\" # <<< Auto-save agent's final weather response\n",
        "    )\n",
        "    print(f\"✅ Root Agent '{root_agent_stateful.name}' created using stateful tool and output_key.\")\n",
        "\n",
        "    # --- Create Runner for this Root Agent & NEW Session Service ---\n",
        "    runner_root_stateful = Runner(\n",
        "        agent=root_agent_stateful,\n",
        "        app_name=APP_NAME,\n",
        "        session_service=session_service_stateful # Use the NEW stateful session service\n",
        "    )\n",
        "    print(f\"✅ Runner created for stateful root agent '{runner_root_stateful.agent.name}' using stateful session service.\")\n",
        "\n",
        "else:\n",
        "    print(\"❌ Cannot create stateful root agent. Prerequisites missing.\")\n",
        "    if not greeting_agent: print(\" - greeting_agent definition missing.\")\n",
        "    if not farewell_agent: print(\" - farewell_agent definition missing.\")\n",
        "    if 'get_weather_stateful' not in globals(): print(\" - get_weather_stateful tool missing.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P394DfSb2aOw"
      },
      "source": [
        "---\n",
        "\n",
        "**4\\. Interact and Test State Flow**\n",
        "\n",
        "Now, let's execute a conversation designed to test the state interactions using the `runner_root_stateful` (associated with our stateful agent and the `session_service_stateful`). We'll use the `call_agent_async` function defined earlier, ensuring we pass the correct runner, user ID (`USER_ID_STATEFUL`), and session ID (`SESSION_ID_STATEFUL`).\n",
        "\n",
        "The conversation flow will be:\n",
        "\n",
        "1.  **Check weather (London):** The `get_weather_stateful` tool should read the initial \"Celsius\" preference from the session state initialized in Section 1. The root agent's final response (the weather report in Celsius) should get saved to `state['last_weather_report']` via the `output_key` configuration.\n",
        "2.  **Manually update state:** We will *directly modify* the state stored within the `InMemorySessionService` instance (`session_service_stateful`).\n",
        "    *   **Why direct modification?** The `session_service.get_session()` method returns a *copy* of the session. Modifying that copy wouldn't affect the state used in subsequent agent runs. For this testing scenario with `InMemorySessionService`, we access the internal `sessions` dictionary to change the *actual* stored state value for `user_preference_temperature_unit` to \"Fahrenheit\". *Note: In real applications, state changes are typically triggered by tools or agent logic returning `EventActions(state_delta=...)`, not direct manual updates.*\n",
        "3.  **Check weather again (New York):** The `get_weather_stateful` tool should now read the updated \"Fahrenheit\" preference from the state and convert the temperature accordingly. The root agent's *new* response (weather in Fahrenheit) will overwrite the previous value in `state['last_weather_report']` due to the `output_key`.\n",
        "4.  **Greet the agent:** Verify that delegation to the `greeting_agent` still works correctly alongside the stateful operations. This interaction will become the *last* response saved by `output_key` in this specific sequence.\n",
        "5.  **Inspect final state:** After the conversation, we retrieve the session one last time (getting a copy) and print its state to confirm the `user_preference_temperature_unit` is indeed \"Fahrenheit\", observe the final value saved by `output_key` (which will be the greeting in this run), and see the `last_city_checked_stateful` value written by the tool.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "WYZfRCp0tFLT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4e2de55-0d20-469d-d73e-26fd163a38b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attempting execution using 'await' (default for notebooks)...\n",
            "\n",
            "--- Testing State: Temp Unit Conversion & output_key ---\n",
            "--- Turn 1: Requesting weather in London (expect Celsius) ---\n",
            "\n",
            ">>> User Query: What's the weather in London?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:google_genai.types:Warning: there are non-text parts in the response: ['function_call'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n",
            "ERROR:asyncio:Unclosed client session\n",
            "client_session: <aiohttp.client.ClientSession object at 0x7905618107a0>\n",
            "ERROR:asyncio:Unclosed connector\n",
            "connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x7905606a3bf0>, 4225.363763725)])']\n",
            "connector: <aiohttp.connector.TCPConnector object at 0x7905618110d0>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Tool: get_weather_stateful called for London ---\n",
            "--- Tool: Reading state 'user_preference_temperature_unit': Celsius ---\n",
            "--- Tool: Generated report in Celsius. Result: {'status': 'success', 'report': 'The weather in London is cloudy with a temperature of 15°C.'} ---\n",
            "--- Tool: Updated state 'last_city_checked_stateful': London ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:asyncio:Unclosed client session\n",
            "client_session: <aiohttp.client.ClientSession object at 0x7905606b86b0>\n",
            "ERROR:asyncio:Unclosed connector\n",
            "connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x7905606a3cb0>, 4226.00223788)])']\n",
            "connector: <aiohttp.connector.TCPConnector object at 0x7905606b9c40>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<<< Agent Response: The weather in London is cloudy with a temperature of 15°C.\n",
            "\n",
            "\n",
            "--- Manually Updating State: Setting unit to Fahrenheit ---\n",
            "--- Stored session state updated. Current 'user_preference_temperature_unit': Fahrenheit ---\n",
            "\n",
            "--- Turn 2: Requesting weather in New York (expect Fahrenheit) ---\n",
            "\n",
            ">>> User Query: Tell me the weather in New York.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:google_genai.types:Warning: there are non-text parts in the response: ['function_call'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n",
            "ERROR:asyncio:Unclosed client session\n",
            "client_session: <aiohttp.client.ClientSession object at 0x790560509b50>\n",
            "ERROR:asyncio:Unclosed connector\n",
            "connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x7905606a3a70>, 4226.745626086)])']\n",
            "connector: <aiohttp.connector.TCPConnector object at 0x7905605091f0>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Tool: get_weather_stateful called for New York ---\n",
            "--- Tool: Reading state 'user_preference_temperature_unit': Fahrenheit ---\n",
            "--- Tool: Generated report in Fahrenheit. Result: {'status': 'success', 'report': 'The weather in New york is sunny with a temperature of 77°F.'} ---\n",
            "--- Tool: Updated state 'last_city_checked_stateful': New York ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:asyncio:Unclosed client session\n",
            "client_session: <aiohttp.client.ClientSession object at 0x790561c0d430>\n",
            "ERROR:asyncio:Unclosed connector\n",
            "connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x7905606a3e30>, 4227.436347694)])']\n",
            "connector: <aiohttp.connector.TCPConnector object at 0x7905606b9550>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<<< Agent Response: The weather in New york is sunny with a temperature of 77°F.\n",
            "\n",
            "\n",
            "--- Turn 3: Sending a greeting ---\n",
            "\n",
            ">>> User Query: Hi!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:google_genai.types:Warning: there are non-text parts in the response: ['function_call'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n",
            "WARNING:google_genai.types:Warning: there are non-text parts in the response: ['function_call'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n",
            "ERROR:asyncio:Unclosed client session\n",
            "client_session: <aiohttp.client.ClientSession object at 0x790560509a30>\n",
            "ERROR:asyncio:Unclosed connector\n",
            "connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x7905606d3050>, 4228.842787828)])']\n",
            "connector: <aiohttp.connector.TCPConnector object at 0x7905606b8b30>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Tool: say_hello called without a specific name (name_arg_value: None) ---\n",
            "<<< Agent Response: Hello there!\n",
            "\n",
            "\n",
            "--- Inspecting Final Session State ---\n",
            "Final Preference: Fahrenheit\n",
            "Final Last Weather Report (from output_key): The weather in New york is sunny with a temperature of 77°F.\n",
            "\n",
            "Final Last City Checked (by tool): New York\n"
          ]
        }
      ],
      "source": [
        "# @title 4. Interact to Test State Flow and output_key\n",
        "import asyncio # Ensure asyncio is imported\n",
        "\n",
        "# Ensure the stateful runner (runner_root_stateful) is available from the previous cell\n",
        "# Ensure call_agent_async, USER_ID_STATEFUL, SESSION_ID_STATEFUL, APP_NAME are defined\n",
        "\n",
        "if 'runner_root_stateful' in globals() and runner_root_stateful:\n",
        "    # Define the main async function for the stateful conversation logic.\n",
        "    # The 'await' keywords INSIDE this function are necessary for async operations.\n",
        "    async def run_stateful_conversation():\n",
        "        print(\"\\n--- Testing State: Temp Unit Conversion & output_key ---\")\n",
        "\n",
        "        # 1. Check weather (Uses initial state: Celsius)\n",
        "        print(\"--- Turn 1: Requesting weather in London (expect Celsius) ---\")\n",
        "        await call_agent_async(query= \"What's the weather in London?\",\n",
        "                               runner=runner_root_stateful,\n",
        "                               user_id=USER_ID_STATEFUL,\n",
        "                               session_id=SESSION_ID_STATEFUL\n",
        "                              )\n",
        "\n",
        "        # 2. Manually update state preference to Fahrenheit - DIRECTLY MODIFY STORAGE\n",
        "        print(\"\\n--- Manually Updating State: Setting unit to Fahrenheit ---\")\n",
        "        try:\n",
        "            # Access the internal storage directly - THIS IS SPECIFIC TO InMemorySessionService for testing\n",
        "            # NOTE: In production with persistent services (Database, VertexAI), you would\n",
        "            # typically update state via agent actions or specific service APIs if available,\n",
        "            # not by direct manipulation of internal storage.\n",
        "            stored_session = session_service_stateful.sessions[APP_NAME][USER_ID_STATEFUL][SESSION_ID_STATEFUL]\n",
        "            stored_session.state[\"user_preference_temperature_unit\"] = \"Fahrenheit\"\n",
        "            # Optional: You might want to update the timestamp as well if any logic depends on it\n",
        "            # import time\n",
        "            # stored_session.last_update_time = time.time()\n",
        "            print(f\"--- Stored session state updated. Current 'user_preference_temperature_unit': {stored_session.state.get('user_preference_temperature_unit', 'Not Set')} ---\") # Added .get for safety\n",
        "        except KeyError:\n",
        "            print(f\"--- Error: Could not retrieve session '{SESSION_ID_STATEFUL}' from internal storage for user '{USER_ID_STATEFUL}' in app '{APP_NAME}' to update state. Check IDs and if session was created. ---\")\n",
        "        except Exception as e:\n",
        "             print(f\"--- Error updating internal session state: {e} ---\")\n",
        "\n",
        "        # 3. Check weather again (Tool should now use Fahrenheit)\n",
        "        # This will also update 'last_weather_report' via output_key\n",
        "        print(\"\\n--- Turn 2: Requesting weather in New York (expect Fahrenheit) ---\")\n",
        "        await call_agent_async(query= \"Tell me the weather in New York.\",\n",
        "                               runner=runner_root_stateful,\n",
        "                               user_id=USER_ID_STATEFUL,\n",
        "                               session_id=SESSION_ID_STATEFUL\n",
        "                              )\n",
        "\n",
        "        # 4. Test basic delegation (should still work)\n",
        "        # This will update 'last_weather_report' again, overwriting the NY weather report\n",
        "        print(\"\\n--- Turn 3: Sending a greeting ---\")\n",
        "        await call_agent_async(query= \"Hi!\",\n",
        "                               runner=runner_root_stateful,\n",
        "                               user_id=USER_ID_STATEFUL,\n",
        "                               session_id=SESSION_ID_STATEFUL\n",
        "                              )\n",
        "\n",
        "    # --- Execute the `run_stateful_conversation` async function ---\n",
        "    # Choose ONE of the methods below based on your environment.\n",
        "\n",
        "    # METHOD 1: Direct await (Default for Notebooks/Async REPLs)\n",
        "    # If your environment supports top-level await (like Colab/Jupyter notebooks),\n",
        "    # it means an event loop is already running, so you can directly await the function.\n",
        "    print(\"Attempting execution using 'await' (default for notebooks)...\")\n",
        "    await run_stateful_conversation()\n",
        "\n",
        "    # METHOD 2: asyncio.run (For Standard Python Scripts [.py])\n",
        "    # If running this code as a standard Python script from your terminal,\n",
        "    # the script context is synchronous. `asyncio.run()` is needed to\n",
        "    # create and manage an event loop to execute your async function.\n",
        "    # To use this method:\n",
        "    # 1. Comment out the `await run_stateful_conversation()` line above.\n",
        "    # 2. Uncomment the following block:\n",
        "    \"\"\"\n",
        "    import asyncio\n",
        "    if __name__ == \"__main__\": # Ensures this runs only when script is executed directly\n",
        "        print(\"Executing using 'asyncio.run()' (for standard Python scripts)...\")\n",
        "        try:\n",
        "            # This creates an event loop, runs your async function, and closes the loop.\n",
        "            asyncio.run(run_stateful_conversation())\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred: {e}\")\n",
        "    \"\"\"\n",
        "\n",
        "    # --- Inspect final session state after the conversation ---\n",
        "    # This block runs after either execution method completes.\n",
        "    print(\"\\n--- Inspecting Final Session State ---\")\n",
        "    final_session = await session_service_stateful.get_session(app_name=APP_NAME,\n",
        "                                                         user_id= USER_ID_STATEFUL,\n",
        "                                                         session_id=SESSION_ID_STATEFUL)\n",
        "    if final_session:\n",
        "        # Use .get() for safer access to potentially missing keys\n",
        "        print(f\"Final Preference: {final_session.state.get('user_preference_temperature_unit', 'Not Set')}\")\n",
        "        print(f\"Final Last Weather Report (from output_key): {final_session.state.get('last_weather_report', 'Not Set')}\")\n",
        "        print(f\"Final Last City Checked (by tool): {final_session.state.get('last_city_checked_stateful', 'Not Set')}\")\n",
        "        # Print full state for detailed view\n",
        "        # print(f\"Full State Dict: {final_session.state.as_dict()}\") # Use as_dict() for clarity\n",
        "    else:\n",
        "        print(\"\\n❌ Error: Could not retrieve final session state.\")\n",
        "\n",
        "else:\n",
        "    print(\"\\n⚠️ Skipping state test conversation. Stateful root agent runner ('runner_root_stateful') is not available.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mqiG4SAX2l8C"
      },
      "source": [
        "---\n",
        "\n",
        "By reviewing the conversation flow and the final session state printout, you can confirm:\n",
        "\n",
        "*   **State Read:** The weather tool (`get_weather_stateful`) correctly read `user_preference_temperature_unit` from state, initially using \"Celsius\" for London.\n",
        "*   **State Update:** The direct modification successfully changed the stored preference to \"Fahrenheit\".\n",
        "*   **State Read (Updated):** The tool subsequently read \"Fahrenheit\" when asked for New York's weather and performed the conversion.\n",
        "*   **Tool State Write:** The tool successfully wrote the `last_city_checked_stateful` (\"New York\" after the second weather check) into the state via `tool_context.state`.\n",
        "*   **Delegation:** The delegation to the `greeting_agent` for \"Hi!\" functioned correctly even after state modifications.\n",
        "*   **`output_key`:** The `output_key=\"last_weather_report\"` successfully saved the root agent's *final* response for *each turn* where the root agent was the one ultimately responding. In this sequence, the last response was the greeting (\"Hello, there!\"), so that overwrote the weather report in the state key.\n",
        "*   **Final State:** The final check confirms the preference persisted as \"Fahrenheit\".\n",
        "\n",
        "You've now successfully integrated session state to personalize agent behavior using `ToolContext`, manually manipulated state for testing `InMemorySessionService`, and observed how `output_key` provides a simple mechanism for saving the agent's last response to state. This foundational understanding of state management is key as we proceed to implement safety guardrails using callbacks in the next steps.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JwTcmu0oaEiI"
      },
      "source": [
        "## Step 5: Adding Safety \\- Input Guardrail with `before_model_callback`\n",
        "\n",
        "Our agent team is becoming more capable, remembering preferences and using tools effectively. However, in real-world scenarios, we often need safety mechanisms to control the agent's behavior *before* potentially problematic requests even reach the core Large Language Model (LLM).\n",
        "\n",
        "ADK provides **Callbacks** – functions that allow you to hook into specific points in the agent's execution lifecycle. The `before_model_callback` is particularly useful for input safety.\n",
        "\n",
        "**What is `before_model_callback`?**\n",
        "\n",
        "* It's a Python function you define that ADK executes *just before* an agent sends its compiled request (including conversation history, instructions, and the latest user message) to the underlying LLM.  \n",
        "* **Purpose:** Inspect the request, modify it if necessary, or block it entirely based on predefined rules.\n",
        "\n",
        "**Common Use Cases:**\n",
        "\n",
        "* **Input Validation/Filtering:** Check if user input meets criteria or contains disallowed content (like PII or keywords).  \n",
        "* **Guardrails:** Prevent harmful, off-topic, or policy-violating requests from being processed by the LLM.  \n",
        "* **Dynamic Prompt Modification:** Add timely information (e.g., from session state) to the LLM request context just before sending.\n",
        "\n",
        "**How it Works:**\n",
        "\n",
        "1. Define a function accepting `callback_context: CallbackContext` and `llm_request: LlmRequest`.  \n",
        "   * `callback_context`: Provides access to agent info, session state (`callback_context.state`), etc.  \n",
        "   * `llm_request`: Contains the full payload intended for the LLM (`contents`, `config`).  \n",
        "2. Inside the function:  \n",
        "   * **Inspect:** Examine `llm_request.contents` (especially the last user message).  \n",
        "   * **Modify (Use Caution):** You *can* change parts of `llm_request`.  \n",
        "   * **Block (Guardrail):** Return an `LlmResponse` object. ADK will send this response back immediately, *skipping* the LLM call for that turn.  \n",
        "   * **Allow:** Return `None`. ADK proceeds to call the LLM with the (potentially modified) request.\n",
        "\n",
        "**In this step, we will:**\n",
        "\n",
        "1. Define a `before_model_callback` function (`block_keyword_guardrail`) that checks the user's input for a specific keyword (\"BLOCK\").  \n",
        "2. Update our stateful root agent (`weather_agent_v4_stateful` from Step 4\\) to use this callback.  \n",
        "3. Create a new runner associated with this updated agent but using the *same stateful session service* to maintain state continuity.  \n",
        "4. Test the guardrail by sending both normal and keyword-containing requests."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7m6zhMv4Zss"
      },
      "source": [
        "---\n",
        "\n",
        "**1\\. Define the Guardrail Callback Function**\n",
        "\n",
        "This function will inspect the last user message within the `llm_request` content. If it finds \"BLOCK\" (case-insensitive), it constructs and returns an `LlmResponse` to block the flow; otherwise, it returns `None`.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "JZay2mbHaHSk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d261abe-8f18-432f-f9eb-9e05adbc37e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ block_keyword_guardrail function defined.\n"
          ]
        }
      ],
      "source": [
        "# @title 1. Define the before_model_callback Guardrail\n",
        "\n",
        "# Ensure necessary imports are available\n",
        "from google.adk.agents.callback_context import CallbackContext\n",
        "from google.adk.models.llm_request import LlmRequest\n",
        "from google.adk.models.llm_response import LlmResponse\n",
        "from google.genai import types # For creating response content\n",
        "from typing import Optional\n",
        "\n",
        "def block_keyword_guardrail(\n",
        "    callback_context: CallbackContext, llm_request: LlmRequest\n",
        ") -> Optional[LlmResponse]:\n",
        "    \"\"\"\n",
        "    Inspects the latest user message for 'BLOCK'. If found, blocks the LLM call\n",
        "    and returns a predefined LlmResponse. Otherwise, returns None to proceed.\n",
        "    \"\"\"\n",
        "    agent_name = callback_context.agent_name # Get the name of the agent whose model call is being intercepted\n",
        "    print(f\"--- Callback: block_keyword_guardrail running for agent: {agent_name} ---\")\n",
        "\n",
        "    # Extract the text from the latest user message in the request history\n",
        "    last_user_message_text = \"\"\n",
        "    if llm_request.contents:\n",
        "        # Find the most recent message with role 'user'\n",
        "        for content in reversed(llm_request.contents):\n",
        "            if content.role == 'user' and content.parts:\n",
        "                # Assuming text is in the first part for simplicity\n",
        "                if content.parts[0].text:\n",
        "                    last_user_message_text = content.parts[0].text\n",
        "                    break # Found the last user message text\n",
        "\n",
        "    print(f\"--- Callback: Inspecting last user message: '{last_user_message_text[:100]}...' ---\") # Log first 100 chars\n",
        "\n",
        "    # --- Guardrail Logic ---\n",
        "    keyword_to_block = \"BLOCK\"\n",
        "    if keyword_to_block in last_user_message_text.upper(): # Case-insensitive check\n",
        "        print(f\"--- Callback: Found '{keyword_to_block}'. Blocking LLM call! ---\")\n",
        "        # Optionally, set a flag in state to record the block event\n",
        "        callback_context.state[\"guardrail_block_keyword_triggered\"] = True\n",
        "        print(f\"--- Callback: Set state 'guardrail_block_keyword_triggered': True ---\")\n",
        "\n",
        "        # Construct and return an LlmResponse to stop the flow and send this back instead\n",
        "        return LlmResponse(\n",
        "            content=types.Content(\n",
        "                role=\"model\", # Mimic a response from the agent's perspective\n",
        "                parts=[types.Part(text=f\"I cannot process this request because it contains the blocked keyword '{keyword_to_block}'.\")],\n",
        "            )\n",
        "            # Note: You could also set an error_message field here if needed\n",
        "        )\n",
        "    else:\n",
        "        # Keyword not found, allow the request to proceed to the LLM\n",
        "        print(f\"--- Callback: Keyword not found. Allowing LLM call for {agent_name}. ---\")\n",
        "        return None # Returning None signals ADK to continue normally\n",
        "\n",
        "print(\"✅ block_keyword_guardrail function defined.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "giawLd9VaI7G"
      },
      "source": [
        "---\n",
        "\n",
        "**2\\. Update Root Agent to Use the Callback**\n",
        "\n",
        "We redefine the root agent, adding the `before_model_callback` parameter and pointing it to our new guardrail function. We'll give it a new version name for clarity.\n",
        "\n",
        "*Important:* We need to redefine the sub-agents (`greeting_agent`, `farewell_agent`) and the stateful tool (`get_weather_stateful`) within this context if they are not already available from previous steps, ensuring the root agent definition has access to all its components."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "IRoMmJ9V_cuH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7fc78ab6-6e45-488b-fb8c-9456bb0e89cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Sub-Agent 'greeting_agent' redefined.\n",
            "✅ Sub-Agent 'farewell_agent' redefined.\n",
            "✅ Root Agent 'weather_agent_v5_model_guardrail' created with before_model_callback.\n",
            "✅ Runner created for guardrail agent 'weather_agent_v5_model_guardrail', using stateful session service.\n"
          ]
        }
      ],
      "source": [
        "# @title 2. Update Root Agent with before_model_callback\n",
        "\n",
        "\n",
        "# --- Redefine Sub-Agents (Ensures they exist in this context) ---\n",
        "greeting_agent = None\n",
        "try:\n",
        "    # Use a defined model constant\n",
        "    greeting_agent = Agent(\n",
        "        model=MODEL_GEMINI_2_0_FLASH,\n",
        "        name=\"greeting_agent\", # Keep original name for consistency\n",
        "        instruction=\"You are the Greeting Agent. Your ONLY task is to provide a friendly greeting using the 'say_hello' tool. Do nothing else.\",\n",
        "        description=\"Handles simple greetings and hellos using the 'say_hello' tool.\",\n",
        "        tools=[say_hello],\n",
        "    )\n",
        "    print(f\"✅ Sub-Agent '{greeting_agent.name}' redefined.\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ Could not redefine Greeting agent. Check Model/API Key ({greeting_agent.model}). Error: {e}\")\n",
        "\n",
        "farewell_agent = None\n",
        "try:\n",
        "    # Use a defined model constant\n",
        "    farewell_agent = Agent(\n",
        "        model=MODEL_GEMINI_2_0_FLASH,\n",
        "        name=\"farewell_agent\", # Keep original name\n",
        "        instruction=\"You are the Farewell Agent. Your ONLY task is to provide a polite goodbye message using the 'say_goodbye' tool. Do not perform any other actions.\",\n",
        "        description=\"Handles simple farewells and goodbyes using the 'say_goodbye' tool.\",\n",
        "        tools=[say_goodbye],\n",
        "    )\n",
        "    print(f\"✅ Sub-Agent '{farewell_agent.name}' redefined.\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ Could not redefine Farewell agent. Check Model/API Key ({farewell_agent.model}). Error: {e}\")\n",
        "\n",
        "\n",
        "# --- Define the Root Agent with the Callback ---\n",
        "root_agent_model_guardrail = None\n",
        "runner_root_model_guardrail = None\n",
        "\n",
        "# Check all components before proceeding\n",
        "if greeting_agent and farewell_agent and 'get_weather_stateful' in globals() and 'block_keyword_guardrail' in globals():\n",
        "\n",
        "    # Use a defined model constant\n",
        "    root_agent_model = MODEL_GEMINI_2_0_FLASH\n",
        "\n",
        "    root_agent_model_guardrail = Agent(\n",
        "        name=\"weather_agent_v5_model_guardrail\", # New version name for clarity\n",
        "        model=root_agent_model,\n",
        "        description=\"Main agent: Handles weather, delegates greetings/farewells, includes input keyword guardrail.\",\n",
        "        instruction=\"You are the main Weather Agent. Provide weather using 'get_weather_stateful'. \"\n",
        "                    \"Delegate simple greetings to 'greeting_agent' and farewells to 'farewell_agent'. \"\n",
        "                    \"Handle only weather requests, greetings, and farewells.\",\n",
        "        tools=[get_weather],\n",
        "        sub_agents=[greeting_agent, farewell_agent], # Reference the redefined sub-agents\n",
        "        output_key=\"last_weather_report\", # Keep output_key from Step 4\n",
        "        before_model_callback=block_keyword_guardrail # <<< Assign the guardrail callback\n",
        "    )\n",
        "    print(f\"✅ Root Agent '{root_agent_model_guardrail.name}' created with before_model_callback.\")\n",
        "\n",
        "    # --- Create Runner for this Agent, Using SAME Stateful Session Service ---\n",
        "    # Ensure session_service_stateful exists from Step 4\n",
        "    if 'session_service_stateful' in globals():\n",
        "        runner_root_model_guardrail = Runner(\n",
        "            agent=root_agent_model_guardrail,\n",
        "            app_name=APP_NAME, # Use consistent APP_NAME\n",
        "            session_service=session_service_stateful # <<< Use the service from Step 4\n",
        "        )\n",
        "        print(f\"✅ Runner created for guardrail agent '{runner_root_model_guardrail.agent.name}', using stateful session service.\")\n",
        "    else:\n",
        "        print(\"❌ Cannot create runner. 'session_service_stateful' from Step 4 is missing.\")\n",
        "\n",
        "else:\n",
        "    print(\"❌ Cannot create root agent with model guardrail. One or more prerequisites are missing or failed initialization:\")\n",
        "    if not greeting_agent: print(\"   - Greeting Agent\")\n",
        "    if not farewell_agent: print(\"   - Farewell Agent\")\n",
        "    if 'get_weather_stateful' not in globals(): print(\"   - 'get_weather_stateful' tool\")\n",
        "    if 'block_keyword_guardrail' not in globals(): print(\"   - 'block_keyword_guardrail' callback\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2EW2LSS4wnz"
      },
      "source": [
        "---\n",
        "\n",
        "**3\\. Interact to Test the Guardrail**\n",
        "\n",
        "Let's test the guardrail's behavior. We'll use the *same session* (`SESSION_ID_STATEFUL`) as in Step 4 to show that state persists across these changes.\n",
        "\n",
        "1. Send a normal weather request (should pass the guardrail and execute).  \n",
        "2. Send a request containing \"BLOCK\" (should be intercepted by the callback).  \n",
        "3. Send a greeting (should pass the root agent's guardrail, be delegated, and execute normally)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "4EnMiXX8aO9n",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c9181ba9-a204-4239-f9ef-63adea6c4b30"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:asyncio:Unclosed client session\n",
            "client_session: <aiohttp.client.ClientSession object at 0x790560509d60>\n",
            "ERROR:asyncio:Unclosed connector\n",
            "connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x7905606a3890>, 4229.891887501)])']\n",
            "connector: <aiohttp.connector.TCPConnector object at 0x7905605096d0>\n",
            "ERROR:asyncio:Unclosed client session\n",
            "client_session: <aiohttp.client.ClientSession object at 0x790560508950>\n",
            "ERROR:asyncio:Unclosed connector\n",
            "connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x7905606a3bf0>, 4228.149422126)])']\n",
            "connector: <aiohttp.connector.TCPConnector object at 0x790560508920>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attempting execution using 'await' (default for notebooks)...\n",
            "\n",
            "--- Testing Model Input Guardrail ---\n",
            "--- Turn 1: Requesting weather in London (expect allowed, Fahrenheit) ---\n",
            "\n",
            ">>> User Query: What is the weather in London?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:google_genai.types:Warning: there are non-text parts in the response: ['function_call'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Callback: block_keyword_guardrail running for agent: weather_agent_v5_model_guardrail ---\n",
            "--- Callback: Inspecting last user message: 'For context:...' ---\n",
            "--- Callback: Keyword not found. Allowing LLM call for weather_agent_v5_model_guardrail. ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:google_genai.types:Warning: there are non-text parts in the response: ['function_call'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n",
            "ERROR:asyncio:Unclosed client session\n",
            "client_session: <aiohttp.client.ClientSession object at 0x790560509190>\n",
            "ERROR:asyncio:Unclosed connector\n",
            "connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x7905606d30b0>, 4231.371308218)])']\n",
            "connector: <aiohttp.connector.TCPConnector object at 0x790560509a00>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Tool: get_weather called for city: London ---\n",
            "--- Callback: block_keyword_guardrail running for agent: weather_agent_v5_model_guardrail ---\n",
            "--- Callback: Inspecting last user message: 'For context:...' ---\n",
            "--- Callback: Keyword not found. Allowing LLM call for weather_agent_v5_model_guardrail. ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:asyncio:Unclosed client session\n",
            "client_session: <aiohttp.client.ClientSession object at 0x790560508440>\n",
            "ERROR:asyncio:Unclosed connector\n",
            "connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x7905606d3110>, 4232.059311827)])']\n",
            "connector: <aiohttp.connector.TCPConnector object at 0x790560508500>\n",
            "ERROR:asyncio:Unclosed client session\n",
            "client_session: <aiohttp.client.ClientSession object at 0x7905605095e0>\n",
            "ERROR:asyncio:Unclosed connector\n",
            "connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x7905606a3a70>, 4230.708506075)])']\n",
            "connector: <aiohttp.connector.TCPConnector object at 0x7905606bbe30>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<<< Agent Response: It's cloudy in London with a temperature of 15°C.\n",
            "\n",
            "\n",
            "--- Turn 2: Requesting with blocked keyword (expect blocked) ---\n",
            "\n",
            ">>> User Query: BLOCK the request for weather in Tokyo\n",
            "--- Callback: block_keyword_guardrail running for agent: weather_agent_v5_model_guardrail ---\n",
            "--- Callback: Inspecting last user message: 'BLOCK the request for weather in Tokyo...' ---\n",
            "--- Callback: Found 'BLOCK'. Blocking LLM call! ---\n",
            "--- Callback: Set state 'guardrail_block_keyword_triggered': True ---\n",
            "<<< Agent Response: I cannot process this request because it contains the blocked keyword 'BLOCK'.\n",
            "\n",
            "--- Turn 3: Sending a greeting (expect allowed) ---\n",
            "\n",
            ">>> User Query: Hello again\n",
            "--- Callback: block_keyword_guardrail running for agent: weather_agent_v5_model_guardrail ---\n",
            "--- Callback: Inspecting last user message: 'Hello again...' ---\n",
            "--- Callback: Keyword not found. Allowing LLM call for weather_agent_v5_model_guardrail. ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:google_genai.types:Warning: there are non-text parts in the response: ['function_call'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n",
            "ERROR:asyncio:Unclosed client session\n",
            "client_session: <aiohttp.client.ClientSession object at 0x79056050bdd0>\n",
            "ERROR:asyncio:Unclosed connector\n",
            "connections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x7905606a3c50>, 4232.741417411)])']\n",
            "connector: <aiohttp.connector.TCPConnector object at 0x79056050bc80>\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ClientError",
          "evalue": "429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 15\\nPlease retry in 1.797765146s.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerMinutePerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.0-flash'}, 'quotaValue': '15'}]}, {'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '1s'}]}}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mClientError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1689036712.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;31m# it means an event loop is already running, so you can directly await the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Attempting execution using 'await' (default for notebooks)...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0;32mawait\u001b[0m \u001b[0mrun_guardrail_test_conversation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;31m# METHOD 2: asyncio.run (For Standard Python Scripts [.py])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1689036712.py\u001b[0m in \u001b[0;36mrun_guardrail_test_conversation\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;31m# 3. Normal greeting (Callback allows root agent, delegation happens)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n--- Turn 3: Sending a greeting (expect allowed) ---\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0;32mawait\u001b[0m \u001b[0minteraction_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Hello again\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;31m# --- Execute the `run_guardrail_test_conversation` async function ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-651208083.py\u001b[0m in \u001b[0;36mcall_agent_async\u001b[0;34m(query, runner, user_id, session_id)\u001b[0m\n\u001b[1;32m     14\u001b[0m   \u001b[0;31m# Key Concept: run_async executes the agent logic and yields Events.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m   \u001b[0;31m# We iterate through events to find the final answer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m   \u001b[0;32masync\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrunner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muser_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msession_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_message\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m       \u001b[0;31m# You can uncomment the line below to see *all* events during execution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m       \u001b[0;31m# print(f\"  [Event] Author: {event.author}, Type: {type(event).__name__}, Final: {event.is_final_response()}, Content: {event.content}\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/adk/runners.py\u001b[0m in \u001b[0;36mrun_async\u001b[0;34m(self, user_id, session_id, new_message, state_delta, run_config)\u001b[0m\n\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m     \u001b[0;32masync\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mAclosing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_run_with_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_message\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0magen\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m       \u001b[0;32masync\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0magen\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m         \u001b[0;32myield\u001b[0m \u001b[0mevent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/adk/runners.py\u001b[0m in \u001b[0;36m_run_with_trace\u001b[0;34m(new_message)\u001b[0m\n\u001b[1;32m    308\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exec_with_plugin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minvocation_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecute\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m         ) as agen:\n\u001b[0;32m--> 310\u001b[0;31m           \u001b[0;32masync\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0magen\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    311\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mevent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/adk/runners.py\u001b[0m in \u001b[0;36m_exec_with_plugin\u001b[0;34m(self, invocation_context, session, execute_fn)\u001b[0m\n\u001b[1;32m    352\u001b[0m       \u001b[0;31m# Step 2: Otherwise continue with normal execution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m       \u001b[0;32masync\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mAclosing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexecute_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minvocation_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0magen\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m         \u001b[0;32masync\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0magen\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    355\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mevent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartial\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m             await self.session_service.append_event(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/adk/runners.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(ctx)\u001b[0m\n\u001b[1;32m    302\u001b[0m         \u001b[0;32masync\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mInvocationContext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAsyncGenerator\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mEvent\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m           \u001b[0;32masync\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mAclosing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0magen\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m             \u001b[0;32masync\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0magen\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m               \u001b[0;32myield\u001b[0m \u001b[0mevent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/adk/agents/base_agent.py\u001b[0m in \u001b[0;36mrun_async\u001b[0;34m(self, parent_context)\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32masync\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mAclosing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_run_with_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0magen\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 248\u001b[0;31m       \u001b[0;32masync\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0magen\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m         \u001b[0;32myield\u001b[0m \u001b[0mevent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/adk/agents/base_agent.py\u001b[0m in \u001b[0;36m_run_with_trace\u001b[0;34m()\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m         \u001b[0;32masync\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mAclosing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_async_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0magen\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m           \u001b[0;32masync\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0magen\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    239\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mevent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/adk/agents/llm_agent.py\u001b[0m in \u001b[0;36m_run_async_impl\u001b[0;34m(self, ctx)\u001b[0m\n\u001b[1;32m    286\u001b[0m   ) -> AsyncGenerator[Event, None]:\n\u001b[1;32m    287\u001b[0m     \u001b[0;32masync\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mAclosing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_llm_flow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0magen\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m       \u001b[0;32masync\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0magen\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    289\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__maybe_save_output_to_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m         \u001b[0;32myield\u001b[0m \u001b[0mevent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/adk/flows/llm_flows/base_llm_flow.py\u001b[0m in \u001b[0;36mrun_async\u001b[0;34m(self, invocation_context)\u001b[0m\n\u001b[1;32m    399\u001b[0m       \u001b[0mlast_event\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m       \u001b[0;32masync\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mAclosing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_one_step_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minvocation_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0magen\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 401\u001b[0;31m         \u001b[0;32masync\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0magen\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    402\u001b[0m           \u001b[0mlast_event\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m           \u001b[0;32myield\u001b[0m \u001b[0mevent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/adk/flows/llm_flows/base_llm_flow.py\u001b[0m in \u001b[0;36m_run_one_step_async\u001b[0;34m(self, invocation_context)\u001b[0m\n\u001b[1;32m    445\u001b[0m             )\n\u001b[1;32m    446\u001b[0m         ) as agen:\n\u001b[0;32m--> 447\u001b[0;31m           \u001b[0;32masync\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0magen\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    448\u001b[0m             \u001b[0;31m# Update the mutable event id to avoid conflict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    449\u001b[0m             \u001b[0mmodel_response_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEvent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/adk/flows/llm_flows/base_llm_flow.py\u001b[0m in \u001b[0;36m_postprocess_async\u001b[0;34m(self, invocation_context, llm_request, llm_response, model_response_event)\u001b[0m\n\u001b[1;32m    537\u001b[0m           )\n\u001b[1;32m    538\u001b[0m       ) as agen:\n\u001b[0;32m--> 539\u001b[0;31m         \u001b[0;32masync\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0magen\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    540\u001b[0m           \u001b[0;32myield\u001b[0m \u001b[0mevent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/adk/flows/llm_flows/base_llm_flow.py\u001b[0m in \u001b[0;36m_postprocess_handle_function_calls_async\u001b[0;34m(self, invocation_context, function_call_event, llm_request)\u001b[0m\n\u001b[1;32m    665\u001b[0m         )\n\u001b[1;32m    666\u001b[0m         \u001b[0;32masync\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mAclosing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent_to_run\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minvocation_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0magen\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 667\u001b[0;31m           \u001b[0;32masync\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0magen\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    668\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mevent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    669\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/adk/agents/base_agent.py\u001b[0m in \u001b[0;36mrun_async\u001b[0;34m(self, parent_context)\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32masync\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mAclosing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_run_with_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0magen\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 248\u001b[0;31m       \u001b[0;32masync\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0magen\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m         \u001b[0;32myield\u001b[0m \u001b[0mevent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/adk/agents/base_agent.py\u001b[0m in \u001b[0;36m_run_with_trace\u001b[0;34m()\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m         \u001b[0;32masync\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mAclosing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_async_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0magen\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m           \u001b[0;32masync\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0magen\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    239\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mevent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/adk/agents/llm_agent.py\u001b[0m in \u001b[0;36m_run_async_impl\u001b[0;34m(self, ctx)\u001b[0m\n\u001b[1;32m    286\u001b[0m   ) -> AsyncGenerator[Event, None]:\n\u001b[1;32m    287\u001b[0m     \u001b[0;32masync\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mAclosing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_llm_flow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0magen\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m       \u001b[0;32masync\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0magen\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    289\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__maybe_save_output_to_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m         \u001b[0;32myield\u001b[0m \u001b[0mevent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/adk/flows/llm_flows/base_llm_flow.py\u001b[0m in \u001b[0;36mrun_async\u001b[0;34m(self, invocation_context)\u001b[0m\n\u001b[1;32m    399\u001b[0m       \u001b[0mlast_event\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m       \u001b[0;32masync\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mAclosing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_one_step_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minvocation_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0magen\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 401\u001b[0;31m         \u001b[0;32masync\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0magen\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    402\u001b[0m           \u001b[0mlast_event\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m           \u001b[0;32myield\u001b[0m \u001b[0mevent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/adk/flows/llm_flows/base_llm_flow.py\u001b[0m in \u001b[0;36m_run_one_step_async\u001b[0;34m(self, invocation_context)\u001b[0m\n\u001b[1;32m    435\u001b[0m         )\n\u001b[1;32m    436\u001b[0m     ) as agen:\n\u001b[0;32m--> 437\u001b[0;31m       \u001b[0;32masync\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mllm_response\u001b[0m \u001b[0;32min\u001b[0m \u001b[0magen\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    438\u001b[0m         \u001b[0;31m# Postprocess after calling the LLM.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m         async with Aclosing(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/adk/flows/llm_flows/base_llm_flow.py\u001b[0m in \u001b[0;36m_call_llm_async\u001b[0;34m(self, invocation_context, llm_request, model_response_event)\u001b[0m\n\u001b[1;32m    765\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m     \u001b[0;32masync\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mAclosing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_call_llm_with_tracing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0magen\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 767\u001b[0;31m       \u001b[0;32masync\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0magen\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    768\u001b[0m         \u001b[0;32myield\u001b[0m \u001b[0mevent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/adk/flows/llm_flows/base_llm_flow.py\u001b[0m in \u001b[0;36m_call_llm_with_tracing\u001b[0;34m()\u001b[0m\n\u001b[1;32m    749\u001b[0m               )\n\u001b[1;32m    750\u001b[0m           ) as agen:\n\u001b[0;32m--> 751\u001b[0;31m             \u001b[0;32masync\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mllm_response\u001b[0m \u001b[0;32min\u001b[0m \u001b[0magen\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    752\u001b[0m               trace_call_llm(\n\u001b[1;32m    753\u001b[0m                   \u001b[0minvocation_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/adk/flows/llm_flows/base_llm_flow.py\u001b[0m in \u001b[0;36m_run_and_handle_error\u001b[0;34m(self, response_generator, invocation_context, llm_request, model_response_event)\u001b[0m\n\u001b[1;32m    941\u001b[0m         \u001b[0;32myield\u001b[0m \u001b[0merror_response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    942\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 943\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mmodel_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    945\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__get_llm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minvocation_context\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mInvocationContext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mBaseLlm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/adk/flows/llm_flows/base_llm_flow.py\u001b[0m in \u001b[0;36m_run_and_handle_error\u001b[0;34m(self, response_generator, invocation_context, llm_request, model_response_event)\u001b[0m\n\u001b[1;32m    925\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    926\u001b[0m       \u001b[0;32masync\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mAclosing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse_generator\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0magen\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 927\u001b[0;31m         \u001b[0;32masync\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mresponse\u001b[0m \u001b[0;32min\u001b[0m \u001b[0magen\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    928\u001b[0m           \u001b[0;32myield\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmodel_error\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/adk/models/google_llm.py\u001b[0m in \u001b[0;36mgenerate_content_async\u001b[0;34m(self, llm_request, stream)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m       response = await self.api_client.aio.models.generate_content(\n\u001b[0m\u001b[1;32m    157\u001b[0m           \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mllm_request\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m           \u001b[0mcontents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mllm_request\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontents\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/genai/models.py\u001b[0m in \u001b[0;36mgenerate_content\u001b[0;34m(self, model, contents, config)\u001b[0m\n\u001b[1;32m   8287\u001b[0m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGenerateContentResponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   8288\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mremaining_remote_calls_afc\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 8289\u001b[0;31m       response = await self._generate_content(\n\u001b[0m\u001b[1;32m   8290\u001b[0m           \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcontents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparsed_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   8291\u001b[0m       )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/genai/models.py\u001b[0m in \u001b[0;36m_generate_content\u001b[0;34m(self, model, contents, config)\u001b[0m\n\u001b[1;32m   7122\u001b[0m     \u001b[0mrequest_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_common\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode_unserializable_types\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 7124\u001b[0;31m     response = await self._api_client.async_request(\n\u001b[0m\u001b[1;32m   7125\u001b[0m         \u001b[0;34m'post'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhttp_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7126\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/genai/_api_client.py\u001b[0m in \u001b[0;36masync_request\u001b[0;34m(self, http_method, path, request_dict, http_options)\u001b[0m\n\u001b[1;32m   1322\u001b[0m     )\n\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1324\u001b[0;31m     result = await self._async_request(\n\u001b[0m\u001b[1;32m   1325\u001b[0m         \u001b[0mhttp_request\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhttp_request\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhttp_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhttp_options\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/genai/_api_client.py\u001b[0m in \u001b[0;36m_async_request\u001b[0;34m(self, http_request, http_options, stream)\u001b[0m\n\u001b[1;32m   1267\u001b[0m         \u001b[0mretry\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtenacity\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAsyncRetrying\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mretry_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1268\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mretry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_async_request_once\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhttp_request\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[no-any-return]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1269\u001b[0;31m     return await self._async_retry(  # type: ignore[no-any-return]\n\u001b[0m\u001b[1;32m   1270\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_async_request_once\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhttp_request\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1271\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tenacity/asyncio/__init__.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0mretry_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRetryCallState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretry_object\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0mdo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretry_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretry_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDoAttempt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tenacity/asyncio/__init__.py\u001b[0m in \u001b[0;36miter\u001b[0;34m(self, retry_state)\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0maction\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretry_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tenacity/_utils.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32masync\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtyping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtyping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtyping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36mexc_check\u001b[0;34m(rs)\u001b[0m\n\u001b[1;32m    416\u001b[0m                 \u001b[0mretry_exc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretry_error_cls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfut\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 418\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mretry_exc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    419\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mretry_exc\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mfut\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNoReturn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_attempt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfailed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_attempt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    447\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    399\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 401\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    402\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m                 \u001b[0;31m# Break a reference cycle with the exception in self._exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tenacity/asyncio/__init__.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDoAttempt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# noqa: B902\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                     \u001b[0mretry_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/genai/_api_client.py\u001b[0m in \u001b[0;36m_async_request_once\u001b[0;34m(self, http_request, stream)\u001b[0m\n\u001b[1;32m   1212\u001b[0m               \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_async_client_session_request_args\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1213\u001b[0m           )\n\u001b[0;32m-> 1214\u001b[0;31m           \u001b[0;32mawait\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAPIError\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_async_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1215\u001b[0m           \u001b[0;32mreturn\u001b[0m \u001b[0mHttpResponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mawait\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m         except (\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/genai/errors.py\u001b[0m in \u001b[0;36mraise_for_async_response\u001b[0;34m(cls, response)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;36m400\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mstatus_code\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mClientError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse_json\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0;36m500\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mstatus_code\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m600\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mServerError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse_json\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mClientError\u001b[0m: 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 15\\nPlease retry in 1.797765146s.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerMinutePerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.0-flash'}, 'quotaValue': '15'}]}, {'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '1s'}]}}"
          ]
        }
      ],
      "source": [
        "# @title 3. Interact to Test the Model Input Guardrail\n",
        "import asyncio # Ensure asyncio is imported\n",
        "\n",
        "# Ensure the runner for the guardrail agent is available\n",
        "if 'runner_root_model_guardrail' in globals() and runner_root_model_guardrail:\n",
        "    # Define the main async function for the guardrail test conversation.\n",
        "    # The 'await' keywords INSIDE this function are necessary for async operations.\n",
        "    async def run_guardrail_test_conversation():\n",
        "        print(\"\\n--- Testing Model Input Guardrail ---\")\n",
        "\n",
        "        # Use the runner for the agent with the callback and the existing stateful session ID\n",
        "        # Define a helper lambda for cleaner interaction calls\n",
        "        interaction_func = lambda query: call_agent_async(query,\n",
        "                                                         runner_root_model_guardrail,\n",
        "                                                         USER_ID_STATEFUL, # Use existing user ID\n",
        "                                                         SESSION_ID_STATEFUL # Use existing session ID\n",
        "                                                        )\n",
        "        # 1. Normal request (Callback allows, should use Fahrenheit from previous state change)\n",
        "        print(\"--- Turn 1: Requesting weather in London (expect allowed, Fahrenheit) ---\")\n",
        "        await interaction_func(\"What is the weather in London?\")\n",
        "\n",
        "        # 2. Request containing the blocked keyword (Callback intercepts)\n",
        "        print(\"\\n--- Turn 2: Requesting with blocked keyword (expect blocked) ---\")\n",
        "        await interaction_func(\"BLOCK the request for weather in Tokyo\") # Callback should catch \"BLOCK\"\n",
        "\n",
        "        # 3. Normal greeting (Callback allows root agent, delegation happens)\n",
        "        print(\"\\n--- Turn 3: Sending a greeting (expect allowed) ---\")\n",
        "        await interaction_func(\"Hello again\")\n",
        "\n",
        "    # --- Execute the `run_guardrail_test_conversation` async function ---\n",
        "    # Choose ONE of the methods below based on your environment.\n",
        "\n",
        "    # METHOD 1: Direct await (Default for Notebooks/Async REPLs)\n",
        "    # If your environment supports top-level await (like Colab/Jupyter notebooks),\n",
        "    # it means an event loop is already running, so you can directly await the function.\n",
        "    print(\"Attempting execution using 'await' (default for notebooks)...\")\n",
        "    await run_guardrail_test_conversation()\n",
        "\n",
        "    # METHOD 2: asyncio.run (For Standard Python Scripts [.py])\n",
        "    # If running this code as a standard Python script from your terminal,\n",
        "    # the script context is synchronous. `asyncio.run()` is needed to\n",
        "    # create and manage an event loop to execute your async function.\n",
        "    # To use this method:\n",
        "    # 1. Comment out the `await run_guardrail_test_conversation()` line above.\n",
        "    # 2. Uncomment the following block:\n",
        "    \"\"\"\n",
        "    import asyncio\n",
        "    if __name__ == \"__main__\": # Ensures this runs only when script is executed directly\n",
        "        print(\"Executing using 'asyncio.run()' (for standard Python scripts)...\")\n",
        "        try:\n",
        "            # This creates an event loop, runs your async function, and closes the loop.\n",
        "            asyncio.run(run_guardrail_test_conversation())\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred: {e}\")\n",
        "    \"\"\"\n",
        "\n",
        "    # --- Inspect final session state after the conversation ---\n",
        "    # This block runs after either execution method completes.\n",
        "    # Optional: Check state for the trigger flag set by the callback\n",
        "    print(\"\\n--- Inspecting Final Session State (After Guardrail Test) ---\")\n",
        "    # Use the session service instance associated with this stateful session\n",
        "    final_session = await session_service_stateful.get_session(app_name=APP_NAME,\n",
        "                                                         user_id=USER_ID_STATEFUL,\n",
        "                                                         session_id=SESSION_ID_STATEFUL)\n",
        "    if final_session:\n",
        "        # Use .get() for safer access\n",
        "        print(f\"Guardrail Triggered Flag: {final_session.state.get('guardrail_block_keyword_triggered', 'Not Set (or False)')}\")\n",
        "        print(f\"Last Weather Report: {final_session.state.get('last_weather_report', 'Not Set')}\") # Should be London weather if successful\n",
        "        print(f\"Temperature Unit: {final_session.state.get('user_preference_temperature_unit', 'Not Set')}\") # Should be Fahrenheit\n",
        "        # print(f\"Full State Dict: {final_session.state.as_dict()}\") # For detailed view\n",
        "    else:\n",
        "        print(\"\\n❌ Error: Could not retrieve final session state.\")\n",
        "\n",
        "else:\n",
        "    print(\"\\n⚠️ Skipping model guardrail test. Runner ('runner_root_model_guardrail') is not available.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5D0KaW-aQ8z"
      },
      "source": [
        "---\n",
        "\n",
        "Observe the execution flow:\n",
        "\n",
        "1. **London Weather:** The callback runs for `weather_agent_v5_model_guardrail`, inspects the message, prints \"Keyword not found. Allowing LLM call.\", and returns `None`. The agent proceeds, calls the `get_weather_stateful` tool (which uses the \"Fahrenheit\" preference from Step 4's state change), and returns the weather. This response updates `last_weather_report` via `output_key`.  \n",
        "2. **BLOCK Request:** The callback runs again for `weather_agent_v5_model_guardrail`, inspects the message, finds \"BLOCK\", prints \"Blocking LLM call\\!\", sets the state flag, and returns the predefined `LlmResponse`. The agent's underlying LLM is *never called* for this turn. The user sees the callback's blocking message.  \n",
        "3. **Hello Again:** The callback runs for `weather_agent_v5_model_guardrail`, allows the request. The root agent then delegates to `greeting_agent`. *Note: The `before_model_callback` defined on the root agent does NOT automatically apply to sub-agents.* The `greeting_agent` proceeds normally, calls its `say_hello` tool, and returns the greeting.\n",
        "\n",
        "You have successfully implemented an input safety layer\\! The `before_model_callback` provides a powerful mechanism to enforce rules and control agent behavior *before* expensive or potentially risky LLM calls are made. Next, we'll apply a similar concept to add guardrails around tool usage itself."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZnH5C0IRaqet"
      },
      "source": [
        "## Step 6: Adding Safety \\- Tool Argument Guardrail (`before_tool_callback`)\n",
        "\n",
        "In Step 5, we added a guardrail to inspect and potentially block user input *before* it reached the LLM. Now, we'll add another layer of control *after* the LLM has decided to use a tool but *before* that tool actually executes. This is useful for validating the *arguments* the LLM wants to pass to the tool.\n",
        "\n",
        "ADK provides the `before_tool_callback` for this precise purpose.\n",
        "\n",
        "**What is `before_tool_callback`?**\n",
        "\n",
        "* It's a Python function executed just *before* a specific tool function runs, after the LLM has requested its use and decided on the arguments.  \n",
        "* **Purpose:** Validate tool arguments, prevent tool execution based on specific inputs, modify arguments dynamically, or enforce resource usage policies.\n",
        "\n",
        "**Common Use Cases:**\n",
        "\n",
        "* **Argument Validation:** Check if arguments provided by the LLM are valid, within allowed ranges, or conform to expected formats.  \n",
        "* **Resource Protection:** Prevent tools from being called with inputs that might be costly, access restricted data, or cause unwanted side effects (e.g., blocking API calls for certain parameters).  \n",
        "* **Dynamic Argument Modification:** Adjust arguments based on session state or other contextual information before the tool runs.\n",
        "\n",
        "**How it Works:**\n",
        "\n",
        "1. Define a function accepting `tool: BaseTool`, `args: Dict[str, Any]`, and `tool_context: ToolContext`.  \n",
        "   * `tool`: The tool object about to be called (inspect `tool.name`).  \n",
        "   * `args`: The dictionary of arguments the LLM generated for the tool.  \n",
        "   * `tool_context`: Provides access to session state (`tool_context.state`), agent info, etc.  \n",
        "2. Inside the function:  \n",
        "   * **Inspect:** Examine the `tool.name` and the `args` dictionary.  \n",
        "   * **Modify:** Change values within the `args` dictionary *directly*. If you return `None`, the tool runs with these modified args.  \n",
        "   * **Block/Override (Guardrail):** Return a **dictionary**. ADK treats this dictionary as the *result* of the tool call, completely *skipping* the execution of the original tool function. The dictionary should ideally match the expected return format of the tool it's blocking.  \n",
        "   * **Allow:** Return `None`. ADK proceeds to execute the actual tool function with the (potentially modified) arguments.\n",
        "\n",
        "**In this step, we will:**\n",
        "\n",
        "1. Define a `before_tool_callback` function (`block_paris_tool_guardrail`) that specifically checks if the `get_weather_stateful` tool is called with the city \"Paris\".  \n",
        "2. If \"Paris\" is detected, the callback will block the tool and return a custom error dictionary.  \n",
        "3. Update our root agent (`weather_agent_v6_tool_guardrail`) to include *both* the `before_model_callback` and this new `before_tool_callback`.  \n",
        "4. Create a new runner for this agent, using the same stateful session service.  \n",
        "5. Test the flow by requesting weather for allowed cities and the blocked city (\"Paris\")."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H5myniS17Q5q"
      },
      "source": [
        "---\n",
        "\n",
        "**1\\. Define the Tool Guardrail Callback Function**\n",
        "\n",
        "This function targets the `get_weather_stateful` tool. It checks the `city` argument. If it's \"Paris\", it returns an error dictionary that looks like the tool's own error response. Otherwise, it allows the tool to run by returning `None`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g4wOLl6aastz"
      },
      "outputs": [],
      "source": [
        "# @title 1. Define the before_tool_callback Guardrail\n",
        "\n",
        "# Ensure necessary imports are available\n",
        "from google.adk.tools.base_tool import BaseTool\n",
        "from google.adk.tools.tool_context import ToolContext\n",
        "from typing import Optional, Dict, Any # For type hints\n",
        "\n",
        "def block_paris_tool_guardrail(\n",
        "    tool: BaseTool, args: Dict[str, Any], tool_context: ToolContext\n",
        ") -> Optional[Dict]:\n",
        "    \"\"\"\n",
        "    Checks if 'get_weather_stateful' is called for 'Paris'.\n",
        "    If so, blocks the tool execution and returns a specific error dictionary.\n",
        "    Otherwise, allows the tool call to proceed by returning None.\n",
        "    \"\"\"\n",
        "    tool_name = tool.name\n",
        "    agent_name = tool_context.agent_name # Agent attempting the tool call\n",
        "    print(f\"--- Callback: block_paris_tool_guardrail running for tool '{tool_name}' in agent '{agent_name}' ---\")\n",
        "    print(f\"--- Callback: Inspecting args: {args} ---\")\n",
        "\n",
        "    # --- Guardrail Logic ---\n",
        "    target_tool_name = \"get_weather_stateful\" # Match the function name used by FunctionTool\n",
        "    blocked_city = \"paris\"\n",
        "\n",
        "    # Check if it's the correct tool and the city argument matches the blocked city\n",
        "    if tool_name == target_tool_name:\n",
        "        city_argument = args.get(\"city\", \"\") # Safely get the 'city' argument\n",
        "        if city_argument and city_argument.lower() == blocked_city:\n",
        "            print(f\"--- Callback: Detected blocked city '{city_argument}'. Blocking tool execution! ---\")\n",
        "            # Optionally update state\n",
        "            tool_context.state[\"guardrail_tool_block_triggered\"] = True\n",
        "            print(f\"--- Callback: Set state 'guardrail_tool_block_triggered': True ---\")\n",
        "\n",
        "            # Return a dictionary matching the tool's expected output format for errors\n",
        "            # This dictionary becomes the tool's result, skipping the actual tool run.\n",
        "            return {\n",
        "                \"status\": \"error\",\n",
        "                \"error_message\": f\"Policy restriction: Weather checks for '{city_argument.capitalize()}' are currently disabled by a tool guardrail.\"\n",
        "            }\n",
        "        else:\n",
        "             print(f\"--- Callback: City '{city_argument}' is allowed for tool '{tool_name}'. ---\")\n",
        "    else:\n",
        "        print(f\"--- Callback: Tool '{tool_name}' is not the target tool. Allowing. ---\")\n",
        "\n",
        "\n",
        "    # If the checks above didn't return a dictionary, allow the tool to execute\n",
        "    print(f\"--- Callback: Allowing tool '{tool_name}' to proceed. ---\")\n",
        "    return None # Returning None allows the actual tool function to run\n",
        "\n",
        "print(\"✅ block_paris_tool_guardrail function defined.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4d01OYJlauSI"
      },
      "source": [
        "---\n",
        "\n",
        "**2\\. Update Root Agent to Use Both Callbacks**\n",
        "\n",
        "We redefine the root agent again (`weather_agent_v6_tool_guardrail`), this time adding the `before_tool_callback` parameter alongside the `before_model_callback` from Step 5\\.\n",
        "\n",
        "*Self-Contained Execution Note:* Similar to Step 5, ensure all prerequisites (sub-agents, tools, `before_model_callback`) are defined or available in the execution context before defining this agent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8BVIl_3uLTZT"
      },
      "outputs": [],
      "source": [
        "# @title 2. Update Root Agent with BOTH Callbacks (Self-Contained)\n",
        "\n",
        "# --- Ensure Prerequisites are Defined ---\n",
        "# (Include or ensure execution of definitions for: Agent, LiteLlm, Runner, ToolContext,\n",
        "#  MODEL constants, say_hello, say_goodbye, greeting_agent, farewell_agent,\n",
        "#  get_weather_stateful, block_keyword_guardrail, block_paris_tool_guardrail)\n",
        "\n",
        "# --- Redefine Sub-Agents (Ensures they exist in this context) ---\n",
        "greeting_agent = None\n",
        "try:\n",
        "    # Use a defined model constant\n",
        "    greeting_agent = Agent(\n",
        "        model=MODEL_GEMINI_2_0_FLASH,\n",
        "        name=\"greeting_agent\", # Keep original name for consistency\n",
        "        instruction=\"You are the Greeting Agent. Your ONLY task is to provide a friendly greeting using the 'say_hello' tool. Do nothing else.\",\n",
        "        description=\"Handles simple greetings and hellos using the 'say_hello' tool.\",\n",
        "        tools=[say_hello],\n",
        "    )\n",
        "    print(f\"✅ Sub-Agent '{greeting_agent.name}' redefined.\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ Could not redefine Greeting agent. Check Model/API Key ({greeting_agent.model}). Error: {e}\")\n",
        "\n",
        "farewell_agent = None\n",
        "try:\n",
        "    # Use a defined model constant\n",
        "    farewell_agent = Agent(\n",
        "        model=MODEL_GEMINI_2_0_FLASH,\n",
        "        name=\"farewell_agent\", # Keep original name\n",
        "        instruction=\"You are the Farewell Agent. Your ONLY task is to provide a polite goodbye message using the 'say_goodbye' tool. Do not perform any other actions.\",\n",
        "        description=\"Handles simple farewells and goodbyes using the 'say_goodbye' tool.\",\n",
        "        tools=[say_goodbye],\n",
        "    )\n",
        "    print(f\"✅ Sub-Agent '{farewell_agent.name}' redefined.\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ Could not redefine Farewell agent. Check Model/API Key ({farewell_agent.model}). Error: {e}\")\n",
        "\n",
        "# --- Define the Root Agent with Both Callbacks ---\n",
        "root_agent_tool_guardrail = None\n",
        "runner_root_tool_guardrail = None\n",
        "\n",
        "if ('greeting_agent' in globals() and greeting_agent and\n",
        "    'farewell_agent' in globals() and farewell_agent and\n",
        "    'get_weather_stateful' in globals() and\n",
        "    'block_keyword_guardrail' in globals() and\n",
        "    'block_paris_tool_guardrail' in globals()):\n",
        "\n",
        "    root_agent_model = MODEL_GEMINI_2_0_FLASH\n",
        "\n",
        "    root_agent_tool_guardrail = Agent(\n",
        "        name=\"weather_agent_v6_tool_guardrail\", # New version name\n",
        "        model=root_agent_model,\n",
        "        description=\"Main agent: Handles weather, delegates, includes input AND tool guardrails.\",\n",
        "        instruction=\"You are the main Weather Agent. Provide weather using 'get_weather_stateful'. \"\n",
        "                    \"Delegate greetings to 'greeting_agent' and farewells to 'farewell_agent'. \"\n",
        "                    \"Handle only weather, greetings, and farewells.\",\n",
        "        tools=[get_weather_stateful],\n",
        "        sub_agents=[greeting_agent, farewell_agent],\n",
        "        output_key=\"last_weather_report\",\n",
        "        before_model_callback=block_keyword_guardrail, # Keep model guardrail\n",
        "        before_tool_callback=block_paris_tool_guardrail # <<< Add tool guardrail\n",
        "    )\n",
        "    print(f\"✅ Root Agent '{root_agent_tool_guardrail.name}' created with BOTH callbacks.\")\n",
        "\n",
        "    # --- Create Runner, Using SAME Stateful Session Service ---\n",
        "    if 'session_service_stateful' in globals():\n",
        "        runner_root_tool_guardrail = Runner(\n",
        "            agent=root_agent_tool_guardrail,\n",
        "            app_name=APP_NAME,\n",
        "            session_service=session_service_stateful # <<< Use the service from Step 4/5\n",
        "        )\n",
        "        print(f\"✅ Runner created for tool guardrail agent '{runner_root_tool_guardrail.agent.name}', using stateful session service.\")\n",
        "    else:\n",
        "        print(\"❌ Cannot create runner. 'session_service_stateful' from Step 4/5 is missing.\")\n",
        "\n",
        "else:\n",
        "    print(\"❌ Cannot create root agent with tool guardrail. Prerequisites missing.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aUo-nu657kc8"
      },
      "source": [
        "---\n",
        "\n",
        "**3\\. Interact to Test the Tool Guardrail**\n",
        "\n",
        "Let's test the interaction flow, again using the same stateful session (`SESSION_ID_STATEFUL`) from the previous steps.\n",
        "\n",
        "1. Request weather for \"New York\": Passes both callbacks, tool executes (using Fahrenheit preference from state).  \n",
        "2. Request weather for \"Paris\": Passes `before_model_callback`. LLM decides to call `get_weather_stateful(city='Paris')`. `before_tool_callback` intercepts, blocks the tool, and returns the error dictionary. Agent relays this error.  \n",
        "3. Request weather for \"London\": Passes both callbacks, tool executes normally."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wpg4fzkLav1-"
      },
      "outputs": [],
      "source": [
        "# @title 3. Interact to Test the Tool Argument Guardrail\n",
        "import asyncio # Ensure asyncio is imported\n",
        "\n",
        "# Ensure the runner for the tool guardrail agent is available\n",
        "if 'runner_root_tool_guardrail' in globals() and runner_root_tool_guardrail:\n",
        "    # Define the main async function for the tool guardrail test conversation.\n",
        "    # The 'await' keywords INSIDE this function are necessary for async operations.\n",
        "    async def run_tool_guardrail_test():\n",
        "        print(\"\\n--- Testing Tool Argument Guardrail ('Paris' blocked) ---\")\n",
        "\n",
        "        # Use the runner for the agent with both callbacks and the existing stateful session\n",
        "        # Define a helper lambda for cleaner interaction calls\n",
        "        interaction_func = lambda query: call_agent_async(query,\n",
        "                                                         runner_root_tool_guardrail,\n",
        "                                                         USER_ID_STATEFUL, # Use existing user ID\n",
        "                                                         SESSION_ID_STATEFUL # Use existing session ID\n",
        "                                                        )\n",
        "        # 1. Allowed city (Should pass both callbacks, use Fahrenheit state)\n",
        "        print(\"--- Turn 1: Requesting weather in New York (expect allowed) ---\")\n",
        "        await interaction_func(\"What's the weather in New York?\")\n",
        "\n",
        "        # 2. Blocked city (Should pass model callback, but be blocked by tool callback)\n",
        "        print(\"\\n--- Turn 2: Requesting weather in Paris (expect blocked by tool guardrail) ---\")\n",
        "        await interaction_func(\"How about Paris?\") # Tool callback should intercept this\n",
        "\n",
        "        # 3. Another allowed city (Should work normally again)\n",
        "        print(\"\\n--- Turn 3: Requesting weather in London (expect allowed) ---\")\n",
        "        await interaction_func(\"Tell me the weather in London.\")\n",
        "\n",
        "    # --- Execute the `run_tool_guardrail_test` async function ---\n",
        "    # Choose ONE of the methods below based on your environment.\n",
        "\n",
        "    # METHOD 1: Direct await (Default for Notebooks/Async REPLs)\n",
        "    # If your environment supports top-level await (like Colab/Jupyter notebooks),\n",
        "    # it means an event loop is already running, so you can directly await the function.\n",
        "    print(\"Attempting execution using 'await' (default for notebooks)...\")\n",
        "    await run_tool_guardrail_test()\n",
        "\n",
        "    # METHOD 2: asyncio.run (For Standard Python Scripts [.py])\n",
        "    # If running this code as a standard Python script from your terminal,\n",
        "    # the script context is synchronous. `asyncio.run()` is needed to\n",
        "    # create and manage an event loop to execute your async function.\n",
        "    # To use this method:\n",
        "    # 1. Comment out the `await run_tool_guardrail_test()` line above.\n",
        "    # 2. Uncomment the following block:\n",
        "    \"\"\"\n",
        "    import asyncio\n",
        "    if __name__ == \"__main__\": # Ensures this runs only when script is executed directly\n",
        "        print(\"Executing using 'asyncio.run()' (for standard Python scripts)...\")\n",
        "        try:\n",
        "            # This creates an event loop, runs your async function, and closes the loop.\n",
        "            asyncio.run(run_tool_guardrail_test())\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred: {e}\")\n",
        "    \"\"\"\n",
        "\n",
        "    # --- Inspect final session state after the conversation ---\n",
        "    # This block runs after either execution method completes.\n",
        "    # Optional: Check state for the tool block trigger flag\n",
        "    print(\"\\n--- Inspecting Final Session State (After Tool Guardrail Test) ---\")\n",
        "    # Use the session service instance associated with this stateful session\n",
        "    final_session = await session_service_stateful.get_session(app_name=APP_NAME,\n",
        "                                                         user_id=USER_ID_STATEFUL,\n",
        "                                                         session_id= SESSION_ID_STATEFUL)\n",
        "    if final_session:\n",
        "        # Use .get() for safer access\n",
        "        print(f\"Tool Guardrail Triggered Flag: {final_session.state.get('guardrail_tool_block_triggered', 'Not Set (or False)')}\")\n",
        "        print(f\"Last Weather Report: {final_session.state.get('last_weather_report', 'Not Set')}\") # Should be London weather if successful\n",
        "        print(f\"Temperature Unit: {final_session.state.get('user_preference_temperature_unit', 'Not Set')}\") # Should be Fahrenheit\n",
        "        # print(f\"Full State Dict: {final_session.state.as_dict()}\") # For detailed view\n",
        "    else:\n",
        "        print(\"\\n❌ Error: Could not retrieve final session state.\")\n",
        "\n",
        "else:\n",
        "    print(\"\\n⚠️ Skipping tool guardrail test. Runner ('runner_root_tool_guardrail') is not available.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6gCcBgMfa1VS"
      },
      "source": [
        "---\n",
        "\n",
        "Analyze the output:\n",
        "\n",
        "1. **New York:** The `before_model_callback` allows the request. The LLM requests `get_weather_stateful`. The `before_tool_callback` runs, inspects the args (`{'city': 'New York'}`), sees it's not \"Paris\", prints \"Allowing tool...\" and returns `None`. The actual `get_weather_stateful` function executes, reads \"Fahrenheit\" from state, and returns the weather report. The agent relays this, and it gets saved via `output_key`.  \n",
        "2. **Paris:** The `before_model_callback` allows the request. The LLM requests `get_weather_stateful(city='Paris')`. The `before_tool_callback` runs, inspects the args, detects \"Paris\", prints \"Blocking tool execution\\!\", sets the state flag, and returns the error dictionary `{'status': 'error', 'error_message': 'Policy restriction...'}`. The actual `get_weather_stateful` function is **never executed**. The agent receives the error dictionary *as if it were the tool's output* and formulates a response based on that error message.  \n",
        "3. **London:** Behaves like New York, passing both callbacks and executing the tool successfully. The new London weather report overwrites the `last_weather_report` in the state.\n",
        "\n",
        "You've now added a crucial safety layer controlling not just *what* reaches the LLM, but also *how* the agent's tools can be used based on the specific arguments generated by the LLM. Callbacks like `before_model_callback` and `before_tool_callback` are essential for building robust, safe, and policy-compliant agent applications."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xYWtGbdI8DZw"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## Conclusion: Your Agent Team is Ready!\n",
        "\n",
        "Congratulations! You've successfully journeyed from building a single, basic weather agent to constructing a sophisticated, multi-agent team using the Agent Development Kit (ADK).\n",
        "\n",
        "**Let's recap what you've accomplished:**\n",
        "\n",
        "*   You started with a **fundamental agent** equipped with a single tool (`get_weather`).\n",
        "*   You explored ADK's **multi-model flexibility** using LiteLLM, running the same core logic with different LLMs like Gemini, GPT-4o, and Claude.\n",
        "*   You embraced **modularity** by creating specialized sub-agents (`greeting_agent`, `farewell_agent`) and enabling **automatic delegation** from a root agent.\n",
        "*   You gave your agents **memory** using **Session State**, allowing them to remember user preferences (`temperature_unit`) and past interactions (`output_key`).\n",
        "*   You implemented crucial **safety guardrails** using both `before_model_callback` (blocking specific input keywords) and `before_tool_callback` (blocking tool execution based on arguments like the city \"Paris\").\n",
        "\n",
        "Through building this progressive Weather Bot team, you've gained hands-on experience with core ADK concepts essential for developing complex, intelligent applications.\n",
        "\n",
        "**Key Takeaways:**\n",
        "\n",
        "*   **Agents & Tools:** The fundamental building blocks for defining capabilities and reasoning. Clear instructions and docstrings are paramount.\n",
        "*   **Runners & Session Services:** The engine and memory management system that orchestrate agent execution and maintain conversational context.\n",
        "*   **Delegation:** Designing multi-agent teams allows for specialization, modularity, and better management of complex tasks. Agent `description` is key for auto-flow.\n",
        "*   **Session State (`ToolContext`, `output_key`):** Essential for creating context-aware, personalized, and multi-turn conversational agents.\n",
        "*   **Callbacks (`before_model`, `before_tool`):** Powerful hooks for implementing safety, validation, policy enforcement, and dynamic modifications *before* critical operations (LLM calls or tool execution).\n",
        "*   **Flexibility (`LiteLlm`):** ADK empowers you to choose the best LLM for the job, balancing performance, cost, and features.\n",
        "\n",
        "**Where to Go Next?**\n",
        "\n",
        "Your Weather Bot team is a great starting point. Here are some ideas to further explore ADK and enhance your application:\n",
        "\n",
        "1.  **Real Weather API:** Replace the `mock_weather_db` in your `get_weather` tool with a call to a real weather API (like OpenWeatherMap, WeatherAPI).\n",
        "2.  **More Complex State:** Store more user preferences (e.g., preferred location, notification settings) or conversation summaries in the session state.\n",
        "3.  **Refine Delegation:** Experiment with different root agent instructions or sub-agent descriptions to fine-tune the delegation logic. Could you add a \"forecast\" agent?\n",
        "4.  **Advanced Callbacks:**\n",
        "    *   Use `after_model_callback` to potentially reformat or sanitize the LLM's response *after* it's generated.\n",
        "    *   Use `after_tool_callback` to process or log the results returned by a tool.\n",
        "    *   Implement `before_agent_callback` or `after_agent_callback` for agent-level entry/exit logic.\n",
        "5.  **Error Handling:** Improve how the agent handles tool errors or unexpected API responses. Maybe add retry logic within a tool.\n",
        "6.  **Persistent Session Storage:** Explore alternatives to `InMemorySessionService` for storing session state persistently (e.g., using databases like Firestore or Cloud SQL – requires custom implementation or future ADK integrations).\n",
        "7.  **Streaming UI:** Integrate your agent team with a web framework (like FastAPI, as shown in the ADK Streaming Quickstart) to create a real-time chat interface.\n",
        "\n",
        "The Agent Development Kit provides a robust foundation for building sophisticated LLM-powered applications. By mastering the concepts covered in this tutorial – tools, state, delegation, and callbacks – you are well-equipped to tackle increasingly complex agentic systems.\n",
        "\n",
        "Happy building!"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}